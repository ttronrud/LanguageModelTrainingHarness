{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0f1092-1fb1-4441-a911-8801fa020fba",
   "metadata": {},
   "source": [
    "# Model Testing Notebook\n",
    "This notebook contains the modelling dependencies, consolidated and without the training code infrastructure. This allows you to create a model with the same dimensions as the one you're training, load the weights from a saved checkpoint, and run prompting tests on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a54d60-4a59-4fc7-9631-92cffd876844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One additional dependency for this notebok\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82844881-ba5c-4843-a7d5-d23a3807e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e08ed7-5dde-4d24-bff1-743d348d9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from IPython import display\n",
    "from torchinfo import summary\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import trange\n",
    "import torch.jit as jit\n",
    "from typing import List\n",
    "import globals as g\n",
    "from modelling_utils import apply_rotary_pos_emb, repeat_kv, Transformer, clean_class_name, pretty_generate\n",
    "from training_utils import plot_grad_flow, get_lr, get_batch, generate_training_data\n",
    "import wandb\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import numpy as np\n",
    "g.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b123a-6a2b-4b72-b004-be6c61a934dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f3a3a-92ef-434b-9a59-d5ce3188e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style word-token embeddings + absolute positional embeddings\n",
    "        Combines word-token-embeddings and word-position-embeddings to transmit\n",
    "        both pieces of information throughout the rest of the model\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"AbsolutePositionalEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Validity assertions\n",
    "        assert config.max_seq_len > 0, f\"Max positional embeddings must be > 0: set max_seq_len positive\"\n",
    "        \n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "        self.max_positional_embeddings = config.max_seq_len\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.wpe = nn.Embedding(self.max_positional_embeddings, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # Batch, seq_len\n",
    "        b, seq_len = idx.size()\n",
    "        assert seq_len <= self.max_positional_embeddings, f\"Input sequence length {seq_len} is greater than max positional embeddings of {self.max_positional_embeddings}\"\n",
    "\n",
    "        tok_emb = self.wte(idx)\n",
    "        pos_emb = self.wpe(torch.arange(0, seq_len, dtype=torch.long, device=idx.device))\n",
    "        emb = self.drop(tok_emb + pos_emb)\n",
    "        return emb\n",
    "\n",
    "class WordTokenEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Word-token-embeddings only\n",
    "        For use in other architectures that handle positional embeddings\n",
    "        differently (e.g. LLama)\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"WordTokenEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.wte(idx)\n",
    "        return self.drop(tok_emb)\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style rotary embeddings adapted from Transformers LLama source\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L94\n",
    "\n",
    "        !!Applied in the self-attention module, not as part of embedding block!!\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"RotaryPositionalEmbedding\"\n",
    "        \n",
    "    def __init__(self, dim, max_position_embeddings=1024, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        # For BC we register cos and sin cached\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        bs,nh,sl,embd = x.size()\n",
    "        position_ids = torch.arange(0,sl, device=x.device).unsqueeze(0)\n",
    "        # float32 precision required according to source\n",
    "        # https://github.com/huggingface/transformers/pull/29285\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        \n",
    "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "class MHASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Multi-Head Attention with rotary embeddings\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Modified to include same manual attention fallback\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(config.n_embd//config.n_head, max_position_embeddings = config.max_seq_len)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "    \n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference in positional embeddings implementation.\n",
    "        This encodes the positions of each part of the inputs into the attention\n",
    "        matrix, as opposed to encoding it directly into the embedding trajectory\n",
    "        \"\"\"\n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "class GQASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Grouped Query Attention with rotary embeddings\n",
    "    https://arxiv.org/pdf/2305.13245\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Causal SA is really just the case where the number of KV heads is equal to the number of Q heads,\n",
    "    but I've separated it here to have a \"simpler\" case available. Compilation *should* take care\n",
    "    of most of the performance differences w/r/t performing matrix multiplications in one go or not\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        assert config.n_head % config.n_kv_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd//self.n_head\n",
    "        self.num_key_value_heads = config.n_kv_head\n",
    "        self.num_key_value_groups = self.n_head // self.num_key_value_heads\n",
    "        \n",
    "        # Q, K, and V aren't identical in size anymore, so can't batch all in one MatMul\n",
    "        self.q_proj = nn.Linear(self.n_embd, self.n_head*self.head_dim, bias = config.bias)\n",
    "        self.k_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        self.v_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(config.n_embd//config.n_head, max_position_embeddings = config.max_seq_len)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "\n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference with rotary MHA -- KV groups are repeated to match\n",
    "        Q dimension before being passed through SDPA\n",
    "        \"\"\"\n",
    "        k = repeat_kv(k, self.num_key_value_groups)\n",
    "        v = repeat_kv(v, self.num_key_value_groups)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A \"fun\" alternative architecture based on RNNs. Also much slower for similar sequence lengths!\n",
    "https://arxiv.org/pdf/2402.19427\n",
    "RNNs are the \"old\" way of doing NLP, and have had some minor resurgence recently.\n",
    "They are much more memory-efficient in the forward pass - since Attention requires\n",
    "memory in some linear (with Flash Attention 2, otherwise it's quadratic) relation\n",
    "with the amount of text passed into it, an RNN requires the same amount of memory\n",
    "for any input lengths, it just takes longer as it passes each step through. \n",
    "\n",
    "First part to define is the RG-LRU cell, which performs a single \"time step\"\n",
    "\"\"\"\n",
    "class RGLRUCell(jit.ScriptModule):\n",
    "    def __init__(self, input_size, hidden_size, epsilon = 1e-10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.c = -8  # Scalar-valued constant\n",
    "        self.eps = epsilon\n",
    "\n",
    "        # Initialize weights\n",
    "        # We're combining the Wa and Wx matrices to reduce the operations \n",
    "        # to a single mat-mul. We can then chunk out the rt and it halves\n",
    "        self.WaWx = nn.Linear(input_size, 2*hidden_size)\n",
    "        \n",
    "        self.Lambda = nn.Parameter(torch.Tensor(hidden_size))  # Λ\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.Lambda.data.uniform_(\n",
    "            torch.logit(torch.tensor(0.9)),\n",
    "            torch.logit(torch.tensor(0.999)),\n",
    "        )    \n",
    "    @jit.script_method    \n",
    "    def forward(self, xt, state):\n",
    "        rt_it = self.WaWx(xt)\n",
    "        rt, it = rt_it.split(self.hidden_size, dim=2)\n",
    "        rt = torch.sigmoid(rt)\n",
    "        it = torch.sigmoid(it)\n",
    "        a = torch.sigmoid(self.Lambda)\n",
    "        at = a**rt\n",
    "        # In my experimentation, an epsilon term was required to prevent\n",
    "        # infinite gradients - because the derivative of (1 - x)^0.5 @ x = 1\n",
    "        # is infinite... ¯\\_(ツ)_/¯ wasn't mentioned in the paper\n",
    "        state = at * state + torch.sqrt(1 - at**2 + self.eps) * (it * xt)\n",
    "        return state\n",
    "\n",
    "\"\"\"\n",
    "The RG-LRU Recurrent Block, which passes the input signal through the cell to modify\n",
    "the hidden state, which is gated by a parallel linear layer.\n",
    "\n",
    "    I opted to ignore the convolutional step. This doesn't perform well enough\n",
    "    on my normie hardware (vs Google's dedicated TPUs with handcrafted CUDA kernels)\n",
    "    to really test vs attention. Might explain why Attention's taken over...\n",
    "\"\"\"\n",
    "class RGLRURecurrentBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.rglru_hidden_size > 0\n",
    "\n",
    "        self.rglru_init_h = torch.zeros(config.rglru_hidden_size)\n",
    "        self.cell = RGLRUCell(config.rglru_hidden_size, config.rglru_hidden_size)\n",
    "\n",
    "        self.in_gate_proj = nn.Linear(config.n_embd, 2*config.rglru_hidden_size)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.rglru_hidden_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        xin = self.in_gate_proj(x)\n",
    "        gate, rnn = xin.chunk(2,-1)\n",
    "        # RNN\n",
    "        inputs = rnn.unbind(1)\n",
    "        state = self.rglru_init_h.repeat(B,1).to(x.device, dtype=x.dtype)\n",
    "        rnn_outs = torch.jit.annotate(List[torch.Tensor], [])\n",
    "        for i in range(T):\n",
    "            state = self.cell(inputs[i], state)\n",
    "            rnn_outs += [state]\n",
    "        rnn = torch.stack(rnn_outs).transpose(0,1)\n",
    "        return self.out_proj(F.gelu(gate)*rnn)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style MLP feed-forward block\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L78\n",
    "\n",
    "        Expands embedding dimension to intermediate size, then collapses\n",
    "        back to pass to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class GatedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    (simplified) LLama-style gated MLP feed-forward block\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L186\n",
    "\n",
    "        More parameters, but gating allows for (theoretically) improved information retention between hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state)))\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style RMS norm\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L74\n",
    "    \"\"\"\n",
    "    def __init__(self, config, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(config.n_embd))\n",
    "\n",
    "    def norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self.norm(x.float()).type_as(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard transformers block structure\n",
    "        Passed attention class, feed forward class, and layer normalization class\n",
    "\n",
    "        Forward involves attention residual followed by feed forward residual to be\n",
    "        passed to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = config.ln_class(config)\n",
    "        self.attn = config.attn_class(config)\n",
    "        self.ln_2 = config.ln_class(config)\n",
    "        self.ff = config.ff_class(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.ff(self.ln_2(x))\n",
    "        return x      \n",
    "\n",
    "class TransformerForCausalLM(Transformer):\n",
    "    \"\"\"\n",
    "    Architecturally-relevant pieces based on Karpathy's nanoGPT\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conf = config\n",
    "        \"\"\"\n",
    "        Transformer decoder stack\n",
    "            Word token embedding sequences are passed through\n",
    "            a series of processing blocks before normalization\n",
    "        \"\"\"\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # Embeddings + dropout\n",
    "            wte = config.emb_class(config),\n",
    "            \n",
    "            # Backbone of Attn->FF blocks\n",
    "            h = nn.ModuleList([\n",
    "                Block(config) for _ in range(config.n_layer)\n",
    "            ]),\n",
    "\n",
    "            # Output hidden state normalization\n",
    "            n_f = config.ln_class(config)\n",
    "        ))\n",
    "        \"\"\"\n",
    "        Language Modelling \"head\"\n",
    "            maps the hidden state output by the transformer decoder stack\n",
    "            to a token from its vocabulary\n",
    "        \"\"\"\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        \"\"\"\n",
    "        \"Weight Tying\"\n",
    "            From Karpathy, which credits https://paperswithcode.com/method/weight-tying\n",
    "            \n",
    "            By making input embeddings and output decodings map within the same space, \n",
    "            the generation of a new token is more like a \"movement\" through this latent space.\n",
    "            The word embeddings then must be clustered by frequency/semantic usage, with n_embd\n",
    "            different potential similarities or differences(/nuances).\n",
    "        \"\"\"\n",
    "        self.transformer.wte.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward method modified from Karpathy\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "\n",
    "        Calculates cross entropy loss if targets are provided. Targets are just\n",
    "        inputs shifted one to the left for foundational training (e.g. which token directly follows the input).\n",
    "    \"\"\"\n",
    "    def forward(self, idx, targets = None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "\n",
    "        hidden_state = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            hidden_state = block(hidden_state)\n",
    "        hidden_state = self.transformer.n_f(hidden_state)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(hidden_state)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(hidden_state[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Default config is a scaled-down version of LLama-3 for ~220M parameters\n",
    "    \"\"\"\n",
    "    max_seq_len = 1024\n",
    "    vocab_size = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer = 12\n",
    "    n_head = 16\n",
    "    n_kv_head = 8\n",
    "    n_embd = 1024\n",
    "    intermediate_size = 3584\n",
    "    dropout = 0.0\n",
    "    bias = False\n",
    "    rotary_encoding = True\n",
    "    \n",
    "    # What component classes are we using?\n",
    "    emb_class = WordTokenEmbeddings\n",
    "    attn_class = GQASelfAttention\n",
    "    ff_class = GatedMLP\n",
    "    ln_class = RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66748623-73b1-4342-b64c-4b9fe2b7038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_default = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b6cbd-0f0b-480f-9396-e89cde73bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerForCausalLM(conf_default).to(g.device, dtype=g.dtype)\n",
    "summary(model, input_size=(1, 1024), dtypes=[torch.long],depth=5)\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5053735-e2cf-4770-9683-675cae81947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available model states to load\n",
    "options = []\n",
    "for f in os.listdir():\n",
    "    if \".pt\" in f[-4:]: # Capture both .pth and .pt\n",
    "        options.append(f)\n",
    "        \n",
    "def load_model_ckpt(fname):\n",
    "    if fname == None:\n",
    "        return\n",
    "    if \".pth\" in fname: # The model's training state, incl. optimizer\n",
    "        model.load_state_dict(torch.load(fname, map_location=torch.device(\"cpu\"))[\"model\"], strict=False)\n",
    "    elif \".pt\" in fname: # Just the model state dictionary\n",
    "        model.load_state_dict(torch.load(fname), strict=False)\n",
    "        \n",
    "interact(load_model_ckpt, \n",
    "         fname = widgets.Dropdown(options=options,description='Model checkpoint:',disabled=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df9d8d5-cfd0-4401-a6b6-930bb58114b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"This is a test\"\"\"\n",
    "\n",
    "output = pretty_generate(test_string, model, dev=g.dev, top_k=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c31ef-4d3f-4e0f-83cf-71f9ae3a04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Get embeddings of each word in the sequence, capture whether it was prompt or output\n",
    "with torch.no_grad():\n",
    "    prompt_traj = model.transformer.wte(\n",
    "        torch.Tensor(enc.encode_ordinary(test_string)).view(-1).long().to(g.dev)\n",
    "    ).cpu().float()\n",
    "    \n",
    "    response_traj = model.transformer.wte(\n",
    "        torch.Tensor(enc.encode_ordinary(output[prompt_traj.size()[0]:])).view(-1).long().to(g.dev)\n",
    "    ).cpu().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ba5eb-e6c2-42dc-9253-e5214828e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_traj.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ef678-9b60-4e7a-9265-deae2c7f48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_traj.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982a077-5931-4ef5-81dc-2dfbfb495412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(prompt_traj)\n",
    "full_traj = pca.transform(torch.cat([prompt_traj, response_traj], dim=0))\n",
    "inp_strings_tok = [enc.decode([t]) for t in enc.encode_ordinary(test_string)]\n",
    "out_strings_tok = [enc.decode([t]) for t in enc.encode_ordinary(output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa56067-2196-4198-bd2a-662f52f407d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "dh = display.display(f, display_id=True)\n",
    "ax = f.add_subplot(projection='3d')\n",
    "\n",
    "prompt_pts = []\n",
    "out_pts = []\n",
    "for q in np.arange(0, full_traj.shape[0]):\n",
    "    if q < len(prompt_traj):\n",
    "        tok = inp_strings_tok[q]\n",
    "        ax.text(full_traj[q,0], full_traj[q,1], full_traj[q,2], tok, color='black')\n",
    "        prompt_pts.append([full_traj[q,0], full_traj[q,1], full_traj[q,2]])\n",
    "    else:\n",
    "        tok = out_strings_tok[q-len(prompt_traj)]\n",
    "        ax.text(full_traj[q,0], full_traj[q,1], full_traj[q,2], tok, color='red')\n",
    "        out_pts.append([full_traj[q,0], full_traj[q,1], full_traj[q,2]])\n",
    "\n",
    "    prompttraj = np.array(prompt_pts)\n",
    "    outtraj = np.array(out_pts)\n",
    "    if prompttraj.shape[0] > 1:\n",
    "        ax.plot(prompttraj[:,0], prompttraj[:,1], prompttraj[:,2], color='black', alpha=0.1)\n",
    "    if outtraj.shape[0] > 1:\n",
    "        ax.plot(outtraj[:,0], outtraj[:,1], outtraj[:,2], color='red', alpha=0.1)\n",
    "\n",
    "    ax.set_xlim(np.min(full_traj[:,0]), np.max(full_traj[:,0]))\n",
    "    ax.set_ylim(np.min(full_traj[:,1]), np.max(full_traj[:,1]))\n",
    "    ax.set_zlim(np.min(full_traj[:,2]), np.max(full_traj[:,2]))\n",
    "\n",
    "    dh.update(f)\n",
    "    time.sleep(1.0/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad858594-8785-4e16-bd66-3ac87f370caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

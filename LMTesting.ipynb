{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0f1092-1fb1-4441-a911-8801fa020fba",
   "metadata": {},
   "source": [
    "# Model Testing Notebook\n",
    "This notebook contains the modelling dependencies, consolidated and without the training code infrastructure. This allows you to create a model with the same dimensions as the one you're training, load the weights from a saved checkpoint, and run prompting tests on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82844881-ba5c-4843-a7d5-d23a3807e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e08ed7-5dde-4d24-bff1-743d348d9968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from IPython import display\n",
    "from torchinfo import summary\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import trange\n",
    "import torch.jit as jit\n",
    "from typing import List\n",
    "import globals as g\n",
    "from modelling_utils import apply_rotary_pos_emb, repeat_kv, Transformer, clean_class_name, pretty_generate\n",
    "from training_utils import plot_grad_flow, get_lr, get_batch, generate_training_data\n",
    "import wandb\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import numpy as np\n",
    "g.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a32519-540b-4b2d-aa86-a993f58c679c",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Since we're using this setup to load and test models trained with the other system, we need a mirrored copy of the model components. They've been collated into a single notebook cell for this purpose, but bear in mind if you modify anything in the training notebook, you'll need to modify it here too!\n",
    "\n",
    "I'm aware best practices would be a single .py dependency or module that contains the definitions, but that would fly in the face of the \"show people the code\" goal. Suffering loudly for your own choices is also software best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119f3a3a-92ef-434b-9a59-d5ce3188e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style word-token embeddings + absolute positional embeddings\n",
    "        Combines word-token-embeddings and word-position-embeddings to transmit\n",
    "        both pieces of information throughout the rest of the model\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"AbsolutePositionalEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Validity assertions\n",
    "        assert config.max_seq_len > 0, f\"Max positional embeddings must be > 0: set max_seq_len positive\"\n",
    "        \n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "        self.max_positional_embeddings = config.max_seq_len\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.wpe = nn.Embedding(self.max_positional_embeddings, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # Batch, seq_len\n",
    "        b, seq_len = idx.size()\n",
    "        assert seq_len <= self.max_positional_embeddings, f\"Input sequence length {seq_len} is greater than max positional embeddings of {self.max_positional_embeddings}\"\n",
    "\n",
    "        tok_emb = self.wte(idx)\n",
    "        pos_emb = self.wpe(torch.arange(0, seq_len, dtype=torch.long, device=idx.device))\n",
    "        emb = self.drop(tok_emb + pos_emb)\n",
    "        return emb\n",
    "\n",
    "class WordTokenEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Word-token-embeddings only\n",
    "        For use in other architectures that handle positional embeddings\n",
    "        differently (e.g. LLama)\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"WordTokenEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.wte(idx)\n",
    "        return self.drop(tok_emb)\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style rotary embeddings adapted from Transformers LLama source\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L94\n",
    "\n",
    "        !!Applied in the self-attention module, not as part of embedding block!!\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"RotaryPositionalEmbedding\"\n",
    "        \n",
    "    def __init__(self, dim, max_position_embeddings=1024, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        # For BC we register cos and sin cached\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        bs,nh,sl,embd = x.size()\n",
    "        position_ids = torch.arange(0,sl, device=x.device).unsqueeze(0)\n",
    "        # float32 precision required according to source\n",
    "        # https://github.com/huggingface/transformers/pull/29285\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        \n",
    "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "class MHASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Multi-Head Attention with rotary embeddings\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Modified to include same manual attention fallback\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(config.n_embd//config.n_head, max_position_embeddings = config.max_seq_len)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "    \n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference in positional embeddings implementation.\n",
    "        This encodes the positions of each part of the inputs into the attention\n",
    "        matrix, as opposed to encoding it directly into the embedding trajectory\n",
    "        \"\"\"\n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "class GQASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Grouped Query Attention with rotary embeddings\n",
    "    https://arxiv.org/pdf/2305.13245\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Causal SA is really just the case where the number of KV heads is equal to the number of Q heads,\n",
    "    but I've separated it here to have a \"simpler\" case available. Compilation *should* take care\n",
    "    of most of the performance differences w/r/t performing matrix multiplications in one go or not\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        assert config.n_head % config.n_kv_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd//self.n_head\n",
    "        self.num_key_value_heads = config.n_kv_head\n",
    "        self.num_key_value_groups = self.n_head // self.num_key_value_heads\n",
    "        \n",
    "        # Q, K, and V aren't identical in size anymore, so can't batch all in one MatMul\n",
    "        self.q_proj = nn.Linear(self.n_embd, self.n_head*self.head_dim, bias = config.bias)\n",
    "        self.k_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        self.v_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(config.n_embd//config.n_head, max_position_embeddings = config.max_seq_len)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "\n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference with rotary MHA -- KV groups are repeated to match\n",
    "        Q dimension before being passed through SDPA\n",
    "        \"\"\"\n",
    "        k = repeat_kv(k, self.num_key_value_groups)\n",
    "        v = repeat_kv(v, self.num_key_value_groups)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A \"fun\" alternative architecture based on RNNs. Also much slower for similar sequence lengths!\n",
    "https://arxiv.org/pdf/2402.19427\n",
    "RNNs are the \"old\" way of doing NLP, and have had some minor resurgence recently.\n",
    "They are much more memory-efficient in the forward pass - since Attention requires\n",
    "memory in some linear (with Flash Attention 2, otherwise it's quadratic) relation\n",
    "with the amount of text passed into it, an RNN requires the same amount of memory\n",
    "for any input lengths, it just takes longer as it passes each step through. \n",
    "\n",
    "First part to define is the RG-LRU cell, which performs a single \"time step\"\n",
    "\"\"\"\n",
    "class RGLRUCell(jit.ScriptModule):\n",
    "    def __init__(self, input_size, hidden_size, epsilon = 1e-10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.c = -8  # Scalar-valued constant\n",
    "        self.eps = epsilon\n",
    "\n",
    "        # Initialize weights\n",
    "        # We're combining the Wa and Wx matrices to reduce the operations \n",
    "        # to a single mat-mul. We can then chunk out the rt and it halves\n",
    "        self.WaWx = nn.Linear(input_size, 2*hidden_size)\n",
    "        \n",
    "        self.Lambda = nn.Parameter(torch.Tensor(hidden_size))  # Λ\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.Lambda.data.uniform_(\n",
    "            torch.logit(torch.tensor(0.9)),\n",
    "            torch.logit(torch.tensor(0.999)),\n",
    "        )    \n",
    "    @jit.script_method    \n",
    "    def forward(self, xt, state):\n",
    "        rt_it = self.WaWx(xt)\n",
    "        rt, it = rt_it.split(self.hidden_size, dim=2)\n",
    "        rt = torch.sigmoid(rt)\n",
    "        it = torch.sigmoid(it)\n",
    "        a = torch.sigmoid(self.Lambda)\n",
    "        at = a**rt\n",
    "        # In my experimentation, an epsilon term was required to prevent\n",
    "        # infinite gradients - because the derivative of (1 - x)^0.5 @ x = 1\n",
    "        # is infinite... ¯\\_(ツ)_/¯ wasn't mentioned in the paper\n",
    "        state = at * state + torch.sqrt(1 - at**2 + self.eps) * (it * xt)\n",
    "        return state\n",
    "\n",
    "\"\"\"\n",
    "The RG-LRU Recurrent Block, which passes the input signal through the cell to modify\n",
    "the hidden state, which is gated by a parallel linear layer.\n",
    "\n",
    "    I opted to ignore the convolutional step. This doesn't perform well enough\n",
    "    on my normie hardware (vs Google's dedicated TPUs with handcrafted CUDA kernels)\n",
    "    to really test vs attention. Might explain why Attention's taken over...\n",
    "\"\"\"\n",
    "class RGLRURecurrentBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.rglru_hidden_size > 0\n",
    "\n",
    "        self.rglru_init_h = torch.zeros(config.rglru_hidden_size)\n",
    "        self.cell = RGLRUCell(config.rglru_hidden_size, config.rglru_hidden_size)\n",
    "\n",
    "        self.in_gate_proj = nn.Linear(config.n_embd, 2*config.rglru_hidden_size)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.rglru_hidden_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        xin = self.in_gate_proj(x)\n",
    "        gate, rnn = xin.chunk(2,-1)\n",
    "        # RNN\n",
    "        inputs = rnn.unbind(1)\n",
    "        state = self.rglru_init_h.repeat(B,1).to(x.device, dtype=x.dtype)\n",
    "        rnn_outs = torch.jit.annotate(List[torch.Tensor], [])\n",
    "        for i in range(T):\n",
    "            state = self.cell(inputs[i], state)\n",
    "            rnn_outs += [state]\n",
    "        rnn = torch.stack(rnn_outs).transpose(0,1)\n",
    "        return self.out_proj(F.gelu(gate)*rnn)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style MLP feed-forward block\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L78\n",
    "\n",
    "        Expands embedding dimension to intermediate size, then collapses\n",
    "        back to pass to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class GatedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    (simplified) LLama-style gated MLP feed-forward block\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L186\n",
    "\n",
    "        More parameters, but gating allows for (theoretically) improved information retention between hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state)))\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style RMS norm\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L74\n",
    "    \"\"\"\n",
    "    def __init__(self, config, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(config.n_embd))\n",
    "\n",
    "    def norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self.norm(x.float()).type_as(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard transformers block structure\n",
    "        Passed attention class, feed forward class, and layer normalization class\n",
    "\n",
    "        Forward involves attention residual followed by feed forward residual to be\n",
    "        passed to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = config.ln_class(config)\n",
    "        self.attn = config.attn_class(config)\n",
    "        self.ln_2 = config.ln_class(config)\n",
    "        self.ff = config.ff_class(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.ff(self.ln_2(x))\n",
    "        return x      \n",
    "\n",
    "class TransformerForCausalLM(Transformer):\n",
    "    \"\"\"\n",
    "    Architecturally-relevant pieces based on Karpathy's nanoGPT\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conf = config\n",
    "        \"\"\"\n",
    "        Transformer decoder stack\n",
    "            Word token embedding sequences are passed through\n",
    "            a series of processing blocks before normalization\n",
    "        \"\"\"\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # Embeddings + dropout\n",
    "            wte = config.emb_class(config),\n",
    "            \n",
    "            # Backbone of Attn->FF blocks\n",
    "            h = nn.ModuleList([\n",
    "                Block(config) for _ in range(config.n_layer)\n",
    "            ]),\n",
    "\n",
    "            # Output hidden state normalization\n",
    "            n_f = config.ln_class(config)\n",
    "        ))\n",
    "        \"\"\"\n",
    "        Language Modelling \"head\"\n",
    "            maps the hidden state output by the transformer decoder stack\n",
    "            to a token from its vocabulary\n",
    "        \"\"\"\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        \"\"\"\n",
    "        \"Weight Tying\"\n",
    "            From Karpathy, which credits https://paperswithcode.com/method/weight-tying\n",
    "            \n",
    "            By making input embeddings and output decodings map within the same space, \n",
    "            the generation of a new token is more like a \"movement\" through this latent space.\n",
    "            The word embeddings then must be clustered by frequency/semantic usage, with n_embd\n",
    "            different potential similarities or differences(/nuances).\n",
    "        \"\"\"\n",
    "        self.transformer.wte.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward method modified from Karpathy\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "\n",
    "        Calculates cross entropy loss if targets are provided. Targets are just\n",
    "        inputs shifted one to the left for foundational training (e.g. which token directly follows the input).\n",
    "    \"\"\"\n",
    "    def forward(self, idx, targets = None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "\n",
    "        hidden_state = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            hidden_state = block(hidden_state)\n",
    "        hidden_state = self.transformer.n_f(hidden_state)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(hidden_state)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(hidden_state[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Default config is a scaled-down version of LLama-3 for ~220M parameters\n",
    "    \"\"\"\n",
    "    max_seq_len = 1024\n",
    "    vocab_size = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer = 12\n",
    "    n_head = 16\n",
    "    n_kv_head = 8\n",
    "    n_embd = 1024\n",
    "    intermediate_size = 3584\n",
    "    dropout = 0.0\n",
    "    bias = False\n",
    "    rotary_encoding = True\n",
    "    \n",
    "    # What component classes are we using?\n",
    "    emb_class = WordTokenEmbeddings\n",
    "    attn_class = GQASelfAttention\n",
    "    ff_class = GatedMLP\n",
    "    ln_class = RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66748623-73b1-4342-b64c-4b9fe2b7038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_smed = Config()\n",
    "conf_smed.n_layer = 30\n",
    "conf_smed.n_embd = 768\n",
    "conf_smed.n_head = 12\n",
    "conf_smed.n_kv_head = 6\n",
    "conf_smed.intermediate_size = 2048\n",
    "\n",
    "conf_default = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3b6cbd-0f0b-480f-9396-e89cde73bee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 233.32M\n",
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "TransformerForCausalLM                                  [1, 1, 50304]             --\n",
      "├─ModuleDict: 1-1                                       --                        --\n",
      "│    └─WordTokenEmbeddings: 2-1                         [1, 1024, 768]            --\n",
      "│    │    └─Embedding: 3-1                              [1, 1024, 768]            38,633,472\n",
      "│    │    └─Dropout: 3-2                                [1, 1024, 768]            --\n",
      "│    └─ModuleList: 2-2                                  --                        --\n",
      "│    │    └─Block: 3-3                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-1                           [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-2                  [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-1                       [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-2                       [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-3                       [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-4         [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-5                       [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-6                      [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-3                           [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-4                          [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-7                       [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-8                         [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-9                       [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-10                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-11                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-4                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-5                           [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-6                  [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-12                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-13                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-14                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-15        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-16                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-17                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-7                           [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-8                          [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-18                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-19                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-20                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-21                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-22                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-5                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-9                           [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-10                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-23                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-24                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-25                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-26        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-27                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-28                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-11                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-12                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-29                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-30                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-31                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-32                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-33                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-6                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-13                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-14                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-34                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-35                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-36                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-37        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-38                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-39                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-15                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-16                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-40                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-41                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-42                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-43                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-44                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-7                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-17                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-18                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-45                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-46                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-47                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-48        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-49                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-50                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-19                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-20                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-51                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-52                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-53                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-54                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-55                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-8                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-21                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-22                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-56                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-57                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-58                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-59        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-60                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-61                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-23                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-24                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-62                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-63                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-64                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-65                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-66                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-9                                  [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-25                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-26                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-67                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-68                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-69                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-70        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-71                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-72                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-27                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-28                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-73                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-74                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-75                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-76                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-77                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-10                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-29                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-30                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-78                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-79                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-80                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-81        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-82                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-83                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-31                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-32                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-84                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-85                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-86                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-87                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-88                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-11                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-33                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-34                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-89                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-90                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-91                      [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-92        [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-93                      [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-94                     [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-35                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-36                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-95                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-96                        [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-97                      [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-98                      [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-99                     [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-12                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-37                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-38                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-100                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-101                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-102                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-103       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-104                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-105                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-39                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-40                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-106                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-107                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-108                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-109                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-110                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-13                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-41                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-42                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-111                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-112                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-113                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-114       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-115                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-116                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-43                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-44                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-117                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-118                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-119                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-120                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-121                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-14                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-45                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-46                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-122                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-123                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-124                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-125       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-126                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-127                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-47                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-48                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-128                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-129                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-130                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-131                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-132                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-15                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-49                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-50                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-133                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-134                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-135                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-136       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-137                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-138                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-51                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-52                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-139                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-140                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-141                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-142                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-143                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-16                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-53                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-54                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-144                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-145                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-146                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-147       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-148                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-149                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-55                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-56                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-150                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-151                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-152                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-153                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-154                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-17                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-57                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-58                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-155                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-156                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-157                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-158       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-159                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-160                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-59                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-60                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-161                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-162                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-163                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-164                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-165                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-18                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-61                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-62                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-166                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-167                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-168                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-169       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-170                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-171                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-63                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-64                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-172                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-173                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-174                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-175                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-176                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-19                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-65                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-66                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-177                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-178                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-179                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-180       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-181                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-182                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-67                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-68                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-183                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-184                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-185                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-186                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-187                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-20                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-69                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-70                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-188                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-189                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-190                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-191       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-192                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-193                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-71                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-72                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-194                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-195                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-196                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-197                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-198                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-21                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-73                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-74                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-199                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-200                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-201                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-202       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-203                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-204                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-75                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-76                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-205                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-206                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-207                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-208                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-209                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-22                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-77                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-78                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-210                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-211                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-212                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-213       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-214                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-215                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-79                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-80                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-216                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-217                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-218                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-219                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-220                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-23                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-81                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-82                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-221                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-222                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-223                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-224       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-225                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-226                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-83                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-84                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-227                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-228                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-229                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-230                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-231                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-24                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-85                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-86                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-232                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-233                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-234                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-235       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-236                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-237                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-87                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-88                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-238                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-239                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-240                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-241                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-242                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-25                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-89                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-90                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-243                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-244                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-245                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-246       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-247                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-248                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-91                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-92                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-249                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-250                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-251                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-252                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-253                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-26                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-93                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-94                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-254                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-255                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-256                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-257       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-258                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-259                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-95                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-96                         [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-260                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-261                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-262                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-263                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-264                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-27                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-97                          [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-98                 [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-265                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-266                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-267                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-268       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-269                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-270                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-99                          [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-100                        [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-271                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-272                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-273                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-274                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-275                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-28                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-101                         [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-102                [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-276                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-277                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-278                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-279       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-280                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-281                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-103                         [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-104                        [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-282                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-283                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-284                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-285                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-286                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-29                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-105                         [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-106                [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-287                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-288                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-289                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-290       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-291                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-292                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-107                         [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-108                        [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-293                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-294                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-295                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-296                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-297                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-30                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-109                         [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-110                [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-298                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-299                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-300                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-301       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-302                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-303                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-111                         [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-112                        [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-304                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-305                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-306                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-307                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-308                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-31                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-113                         [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-114                [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-309                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-310                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-311                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-312       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-313                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-314                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-115                         [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-116                        [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-315                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-316                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-317                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-318                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-319                    [1, 1024, 768]            --\n",
      "│    │    └─Block: 3-32                                 [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-117                         [1, 1024, 768]            768\n",
      "│    │    │    └─GQASelfAttention: 4-118                [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-320                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Linear: 5-321                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─Linear: 5-322                     [1, 1024, 384]            294,912\n",
      "│    │    │    │    └─LlamaRotaryEmbedding: 5-323       [1, 1024, 64]             --\n",
      "│    │    │    │    └─Linear: 5-324                     [1, 1024, 768]            589,824\n",
      "│    │    │    │    └─Dropout: 5-325                    [1, 1024, 768]            --\n",
      "│    │    │    └─RMSNorm: 4-119                         [1, 1024, 768]            768\n",
      "│    │    │    └─GatedMLP: 4-120                        [1, 1024, 768]            --\n",
      "│    │    │    │    └─Linear: 5-326                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─SiLU: 5-327                       [1, 1024, 2048]           --\n",
      "│    │    │    │    └─Linear: 5-328                     [1, 1024, 2048]           1,572,864\n",
      "│    │    │    │    └─Linear: 5-329                     [1, 1024, 768]            1,572,864\n",
      "│    │    │    │    └─Dropout: 5-330                    [1, 1024, 768]            --\n",
      "│    └─RMSNorm: 2-3                                     [1, 1024, 768]            768\n",
      "├─Linear: 1-2                                           [1, 1, 50304]             38,633,472\n",
      "=========================================================================================================\n",
      "Total params: 271,955,712\n",
      "Trainable params: 271,955,712\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 271.96\n",
      "=========================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1076.04\n",
      "Params size (MB): 543.91\n",
      "Estimated Total Size (MB): 1619.96\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = TransformerForCausalLM(conf_smed).to(g.device, dtype=g.dtype)\n",
    "model_stats = summary(model, input_size=(1, 1024), dtypes=[torch.long],depth=5)\n",
    "print(str(model_stats))\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5053735-e2cf-4770-9683-675cae81947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf2e211a4ab450bbbb612be3211c647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Model checkpoint:', options=('ckpt60000.pth', 'ckpt80000_220M_defa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.load_model_ckpt(fname)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of available model states to load\n",
    "options = []\n",
    "for f in os.listdir():\n",
    "    if \".pt\" in f[-4:]: # Capture both .pth and .pt\n",
    "        options.append(f)\n",
    "\"\"\"\n",
    "Given a model checkpoint filename, (attempt to) load it into the model\n",
    "you've just built and compiled. \n",
    "Our checkpoint data includes optimizer states, etc. so we just want the model's\n",
    "state dictionary for this.\n",
    "\"\"\"\n",
    "def load_model_ckpt(fname):\n",
    "    if fname == None:\n",
    "        return\n",
    "    if \".pth\" in fname: # The model's training state, incl. optimizer\n",
    "        model.load_state_dict(torch.load(fname, map_location = torch.device(\"cpu\"), weights_only = True)[\"model\"], strict = False)\n",
    "    elif \".pt\" in fname: # Just the model state dictionary\n",
    "        model.load_state_dict(torch.load(fname, weights_only = True), strict = False)\n",
    "    \n",
    "interact(load_model_ckpt, \n",
    "         fname = widgets.Dropdown(options=options,description='Model checkpoint:',disabled=False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df9d8d5-cfd0-4401-a6b6-930bb58114b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with random seed 393853\n",
      "\u001b[98m ## Introduction \n",
      "\n",
      "Medical device product development stands to benefit massively from the integration of machine learning (ML) techniques and strategies. ML encompasses a vast suite of technologies that, when implemented properly, can glean models from large quantities of data and perform analysis much faster than is possible for any human. \n",
      "\n",
      "The best practices described here are intentionally high-level and general and were based on the work done by Assadi et al., with several Toronto-based hospitals and biomedical engineering groups. Their framework is based on the application of the principles of systems and software engineering. It defines four phases – Inception, Preparation, Development, and Integration. In each phase, three domains of solution integration are considered – The Human, The Technical System, and The Environment. \n",
      "\n",
      "The Human domain consists\u001b[00m\u001b[92m  of the process of gathering data, analyzing data, and deciding what to do with the data. The human element includes the person who is performing the analysis. This may be a patient, a family member, or a colleague.\n",
      "The Technical domain is the process of creating the model. It may be the development of the data, the implementation of the model, or the testing of the model. The technical domain includes the development of the data, the development of the data management system, and the testing of the model.\n",
      "The Environmental domain consists of the process of analyzing the data. It may be the analysis of the data, the interpretation of the data, the analysis of the data, and the evaluation of the data. The environmental domain may include the analysis of the data, the interpretation of the data, and the evaluation of the data.\n",
      "The Human, Technical, and Environmental domains are interrelated. The human element includes the person who is performing the analysis. The technical element includes the development\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"This is a test\"\"\"\n",
    "\n",
    "output = pretty_generate(test_string, model, dev=g.dev, min_p = 0.25, temperature=1.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33912d35-0e09-4804-9b23-776aae602b9b",
   "metadata": {},
   "source": [
    "## Language as a signal\n",
    "These next few cells are meant to demonstrate how language \"moves\" through the model's semantic/embedding space. Since we're ill-equipped to deal with 1000+ dimensional spaces, we'll use principal component analysis (PCA) to determine the top three axes along which the prompt tokens are most spread out. This is likely not ideal, but will at least illustrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c31ef-4d3f-4e0f-83cf-71f9ae3a04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Get embeddings of each word in the sequence, capture whether it was prompt or output\n",
    "with torch.no_grad():\n",
    "    prompt_traj = model.transformer.wte(\n",
    "        torch.Tensor(enc.encode_ordinary(test_string)).view(-1).long().to(g.dev)\n",
    "    ).cpu().float()\n",
    "    \n",
    "    response_traj = model.transformer.wte(\n",
    "        torch.Tensor(enc.encode_ordinary(output[prompt_traj.size()[0]:])).view(-1).long().to(g.dev)\n",
    "    ).cpu().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7982a077-5931-4ef5-81dc-2dfbfb495412",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(prompt_traj)\n",
    "full_traj = pca.transform(torch.cat([prompt_traj, response_traj], dim=0))\n",
    "inp_strings_tok = [enc.decode([t]) for t in enc.encode_ordinary(test_string)]\n",
    "out_strings_tok = [enc.decode([t]) for t in enc.encode_ordinary(output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa56067-2196-4198-bd2a-662f52f407d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "dh = display.display(f, display_id=True)\n",
    "ax = f.add_subplot(projection='3d')\n",
    "\n",
    "prompt_pts = []\n",
    "out_pts = []\n",
    "for q in np.arange(0, full_traj.shape[0]):\n",
    "    ax.clear()\n",
    "    if q < len(prompt_traj):\n",
    "        tok = inp_strings_tok[q]\n",
    "        ax.text(full_traj[q,0], full_traj[q,1], full_traj[q,2], tok, color='black')\n",
    "        prompt_pts.append([full_traj[q,0], full_traj[q,1], full_traj[q,2]])\n",
    "    else:\n",
    "        tok = out_strings_tok[q-len(prompt_traj)]\n",
    "        ax.text(full_traj[q,0], full_traj[q,1], full_traj[q,2], tok, color='red')\n",
    "        out_pts.append([full_traj[q,0], full_traj[q,1], full_traj[q,2]])\n",
    "\n",
    "    prompttraj = np.array(prompt_pts)\n",
    "    outtraj = np.array(out_pts)\n",
    "    if prompttraj.shape[0] > 1:\n",
    "        ax.plot(prompttraj[:,0], prompttraj[:,1], prompttraj[:,2], color='black', alpha=0.1)\n",
    "    if outtraj.shape[0] > 1:\n",
    "        ax.plot(outtraj[:,0], outtraj[:,1], outtraj[:,2], color='red', alpha=0.1)\n",
    "\n",
    "    ax.set_xlim(np.min(full_traj[:,0]), np.max(full_traj[:,0]))\n",
    "    ax.set_ylim(np.min(full_traj[:,1]), np.max(full_traj[:,1]))\n",
    "    ax.set_zlim(np.min(full_traj[:,2]), np.max(full_traj[:,2]))\n",
    "\n",
    "    dh.update(f)\n",
    "    time.sleep(1.0/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad858594-8785-4e16-bd66-3ac87f370caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

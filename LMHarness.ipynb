{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf24a32-9b32-4913-9fc1-221ad8b05930",
   "metadata": {},
   "source": [
    "## Notebook Organization Notes and Errata\n",
    "This notebook is meant to be both an educational tool, and lightweight-yet-fully-functional utility for training language models. These are two generally opposing goals, and this tension has motivated some of the choices and sacrifices I've made.\n",
    "\n",
    "* This notebook is organized in a bottom-up approach. Lower-level components, such as specific embedding, attention, and feed-forward classes are defined first, and used later, so that one could feasibly shift+enter their way through the entire notebook without issue. This choice was also motivated by the 'educational' aspect of this notebook - the individual components and their structure/code are important, and we build up from there.\n",
    "\n",
    "* I have presented, in many cases, several different classes for each component. This is an intentional choice. One can (hopefully, if I've done my job right) see the similarities between the methodologies in spite of their minor differences. My focus on the Llama(-esque) architecture as the \"modern\" approach is primarily dictated by the ubiquity of this architecture and their specific metholodological choices across different language models, and the clear success it has shown at *modelling language*. I've put some effort into making the components more interchangeable than their original forms, and I don't think I've introduced any significant errors (famous last words).\n",
    "\n",
    "* For clarity of code in the notebook itself, non-architectural utilities and other methods have been moved to separate files, so that the focus of the code presented here is *how* the information is modified as it flows through the model. I've also neglected to include certain optimizations and additions (e.g. KV caches, dynamic RoPE scaling, etc) to avoid adding undue complexity. If you want to use transformer-based models for any task that requires performance, you have *plenty* of other frameworks to use.\n",
    "\n",
    "* The training loop method uses several values set in globals.py (accessed via a widget interface), which it copies the values of so that they remain consistent during a single training run, even if the widget is changed. Widget controls aren't incredibly fine-grained, but are serviceable. This system will also \"allow\" you to use settings that will fail/use too much VRAM. Settings-tuning is on my radar, but is out of scope for now.\n",
    "\n",
    "* It's *very* important to note that a vast majority of the code in this notebook (and in the utility files) is adapted from other sources. I've done my best to note those sources, and to explain any modifications I've made. This is the nature of Software Development, and it's my hope that the work I've done here will empower people to dive deeper into the sources themselves, armed with a better understanding of how it's working at the lowest level.\n",
    "\n",
    "<sup>Thorold Tronrud, ML/AI Software Engineer, StarFish Medical</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2eab4c-e073-4aa5-bef3-537d26a83399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies - skip if you're coming into this already set up\n",
    "!pip install --upgrade torch numpy datasets tiktoken wandb tqdm torchinfo matplotlib ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b4be6-3c0c-4768-a1b9-347014c84294",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5dde307-ea7c-4094-b541-4685436a3d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from IPython import display\n",
    "from torchinfo import summary\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import trange\n",
    "import torch.jit as jit\n",
    "from typing import List\n",
    "import globals as g\n",
    "from modelling_utils import apply_rotary_pos_emb, repeat_kv, Transformer, clean_class_name, pretty_generate\n",
    "from training_utils import plot_grad_flow, get_lr, get_batch, generate_training_data\n",
    "import wandb\n",
    "g.get_device()\n",
    "\n",
    "# If running on machine with multiple GPUs, you can set a specific device like this:\n",
    "# g.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b116ee-9039-4a2f-98e3-381bdba062ce",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Embeddings convert a stream of discrete tokens into a trajectory through a semantic space. In many cases, these word-token-embeddings are mixed with positional embeddings -- either absolute and learned (e.g. GPT2) or sinusoidal across the sequence length (e.g. LLama, etc). Positional embeddings provide the model information about relative placement of different tokens, and distance between them, which is sometimes useful in language. \n",
    "\n",
    "\"NoPE\", or \"No Positional Embeddings\" has shown success, however, so YMMV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45a193-d735-4a86-bdfd-63a19cce58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style word-token embeddings + absolute positional embeddings\n",
    "        Combines word-token-embeddings and word-position-embeddings to transmit\n",
    "        both pieces of information throughout the rest of the model\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"AbsolutePositionalEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Validity assertions\n",
    "        assert config.max_seq_len > 0, f\"Max positional embeddings must be > 0: set max_seq_len positive\"\n",
    "        \n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "        self.max_positional_embeddings = config.max_seq_len\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.wpe = nn.Embedding(self.max_positional_embeddings, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # Batch, seq_len\n",
    "        b, seq_len = idx.size()\n",
    "        assert seq_len <= self.max_positional_embeddings, f\"Input sequence length {seq_len} is greater than max positional embeddings of {self.max_positional_embeddings}\"\n",
    "\n",
    "        tok_emb = self.wte(idx)\n",
    "        pos_emb = self.wpe(torch.arange(0, seq_len, dtype=torch.long, device=idx.device))\n",
    "        emb = self.drop(tok_emb + pos_emb)\n",
    "        return emb\n",
    "\n",
    "class WordTokenEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Word-token-embeddings only\n",
    "        For use in other architectures that handle positional embeddings\n",
    "        differently (e.g. LLama)\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"WordTokenEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.wte(idx)\n",
    "        return self.drop(tok_emb)\n",
    "\n",
    "class LlamaRotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style rotary embeddings adapted from Transformers LLama source\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L94\n",
    "\n",
    "        !!Applied in the self-attention module, not as part of embedding block!!\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"RotaryPositionalEmbedding\"\n",
    "        \n",
    "    def __init__(self, dim, max_position_embeddings=1024, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        # For BC we register cos and sin cached\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        bs,nh,sl,embd = x.size()\n",
    "        position_ids = torch.arange(0,sl, device=x.device).unsqueeze(0)\n",
    "        # float32 precision required according to source\n",
    "        # https://github.com/huggingface/transformers/pull/29285\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        \n",
    "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171c121-c2e9-484e-996e-06a6bbe93faa",
   "metadata": {},
   "source": [
    "## Attention\n",
    "Attention is the main ingredient that makes Transformers so powerful. An attention block is composed of different attention \"heads\", which control the flow of information, interact between model layers, and each add their own information to the residual.\n",
    "\n",
    "Each attention head computes \"query\", \"key\", and \"value\" vectors for each input token. The attention pattern is the dot product of the query and key vectors, giving a value for the effect each token has on every other token. Finally, multiplying this token-token effect matrix by the value matrix gives a result vector for the head.\n",
    "\n",
    "The results for all heads are stacked side-by-side and passed through a final output layer, which produces the block's predicted \"change\" to the semantic position of all the tokens.\n",
    "\n",
    "[This article](https://transformer-circuits.pub/2021/framework/index.html) does an amazing job breaking down the function of attention heads, albeit with a slightly different model for their behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470da226-9029-4e02-9d7c-06a78d6d9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Multi-Head Attention with rotary embeddings\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Modified to include same manual attention fallback\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(config.n_embd//config.n_head, max_position_embeddings = config.max_seq_len)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "    \n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference in positional embeddings implementation.\n",
    "        This encodes the positions of each part of the inputs into the attention\n",
    "        matrix, as opposed to encoding it directly into the embedding trajectory\n",
    "        \"\"\"\n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "class GQASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Grouped Query Attention with rotary embeddings\n",
    "    https://arxiv.org/pdf/2305.13245\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Causal SA is really just the case where the number of KV heads is equal to the number of Q heads,\n",
    "    but I've separated it here to have a \"simpler\" case available. Compilation *should* take care\n",
    "    of most of the performance differences w/r/t performing matrix multiplications in one go or not\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        assert config.n_head % config.n_kv_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd//self.n_head\n",
    "        self.num_key_value_heads = config.n_kv_head\n",
    "        self.num_key_value_groups = self.n_head // self.num_key_value_heads\n",
    "        \n",
    "        # Q, K, and V aren't identical in size anymore, so can't batch all in one MatMul\n",
    "        self.q_proj = nn.Linear(self.n_embd, self.n_head*self.head_dim, bias = config.bias)\n",
    "        self.k_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        self.v_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(config.n_embd//config.n_head, max_position_embeddings = config.max_seq_len)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "\n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference with rotary MHA -- KV groups are repeated to match\n",
    "        Q dimension before being passed through SDPA\n",
    "        \"\"\"\n",
    "        k = repeat_kv(k, self.num_key_value_groups)\n",
    "        v = repeat_kv(v, self.num_key_value_groups)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A \"fun\" alternative architecture based on RNNs. Also much slower for similar sequence lengths!\n",
    "https://arxiv.org/pdf/2402.19427\n",
    "RNNs are the \"old\" way of doing NLP, and have had some minor resurgence recently.\n",
    "They are much more memory-efficient in the forward pass - since Attention requires\n",
    "memory in some linear (with Flash Attention 2, otherwise it's quadratic) relation\n",
    "with the amount of text passed into it, an RNN requires the same amount of memory\n",
    "for any input lengths, it just takes longer as it passes each step through. \n",
    "\n",
    "First part to define is the RG-LRU cell, which performs a single \"time step\"\n",
    "\"\"\"\n",
    "class RGLRUCell(jit.ScriptModule):\n",
    "    def __init__(self, input_size, hidden_size, epsilon = 1e-10):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.c = -8  # Scalar-valued constant\n",
    "        self.eps = epsilon\n",
    "\n",
    "        # Initialize weights\n",
    "        # We're combining the Wa and Wx matrices to reduce the operations \n",
    "        # to a single mat-mul. We can then chunk out the rt and it halves\n",
    "        self.WaWx = nn.Linear(input_size, 2*hidden_size)\n",
    "        \n",
    "        self.Lambda = nn.Parameter(torch.Tensor(hidden_size))  # Λ\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.Lambda.data.uniform_(\n",
    "            torch.logit(torch.tensor(0.9)),\n",
    "            torch.logit(torch.tensor(0.999)),\n",
    "        )    \n",
    "    @jit.script_method    \n",
    "    def forward(self, xt, state):\n",
    "        rt_it = self.WaWx(xt)\n",
    "        rt, it = rt_it.split(self.hidden_size, dim=2)\n",
    "        rt = torch.sigmoid(rt)\n",
    "        it = torch.sigmoid(it)\n",
    "        a = torch.sigmoid(self.Lambda)\n",
    "        at = a**rt\n",
    "        # In my experimentation, an epsilon term was required to prevent\n",
    "        # infinite gradients - because the derivative of (1 - x)^0.5 @ x = 1\n",
    "        # is infinite... ¯\\_(ツ)_/¯ wasn't mentioned in the paper\n",
    "        state = at * state + torch.sqrt(1 - at**2 + self.eps) * (it * xt)\n",
    "        return state\n",
    "\n",
    "\"\"\"\n",
    "The RG-LRU Recurrent Block, which passes the input signal through the cell to modify\n",
    "the hidden state, which is gated by a parallel linear layer.\n",
    "\n",
    "    I opted to ignore the convolutional step. This doesn't perform well enough\n",
    "    on my normie hardware (vs Google's dedicated TPUs with handcrafted CUDA kernels)\n",
    "    to really test vs attention. Might explain why Attention's taken over...\n",
    "\"\"\"\n",
    "class RGLRURecurrentBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.rglru_hidden_size > 0\n",
    "\n",
    "        self.rglru_init_h = torch.zeros(config.rglru_hidden_size)\n",
    "        self.cell = RGLRUCell(config.rglru_hidden_size, config.rglru_hidden_size)\n",
    "\n",
    "        self.in_gate_proj = nn.Linear(config.n_embd, 2*config.rglru_hidden_size)\n",
    "\n",
    "        self.out_proj = nn.Linear(config.rglru_hidden_size, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        xin = self.in_gate_proj(x)\n",
    "        gate, rnn = xin.chunk(2,-1)\n",
    "        # RNN\n",
    "        inputs = rnn.unbind(1)\n",
    "        state = self.rglru_init_h.repeat(B,1).to(x.device, dtype=x.dtype)\n",
    "        rnn_outs = torch.jit.annotate(List[torch.Tensor], [])\n",
    "        for i in range(T):\n",
    "            state = self.cell(inputs[i], state)\n",
    "            rnn_outs += [state]\n",
    "        rnn = torch.stack(rnn_outs).transpose(0,1)\n",
    "        return self.out_proj(F.gelu(gate)*rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc99d58-fb4c-435c-b15b-858abb9a3d60",
   "metadata": {},
   "source": [
    "## Feed Forward/MLP Block\n",
    "\n",
    "The feed-forward block is generally a shallow multi-layer perceptron (MLP) that calculates a residual on top of the output from the attention block. This further adjusts the direction of the next \"step\" through the model, although it is not [well understood](https://transformer-circuits.pub/2021/framework/index.html#additional-intuition) what these adjustments directly correspond to (or even, in fact, if they do correspond to any *specific* features outside of rare cases).\n",
    "\n",
    "The slightly more complex gated MLP provides a more elegant channel for specific pieces of information passed from the attention block to be ignored, by allowing a gate value to be zero simply by generating a negative number in the gate projection layer, which then fully cancels out information without needing to be the exact negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fe6d5-878b-4d68-a715-c7b81640dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style MLP feed-forward block\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L78\n",
    "\n",
    "        Expands embedding dimension to intermediate size, then collapses\n",
    "        back to pass to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class GatedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    (simplified) LLama-style gated MLP feed-forward block\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L186\n",
    "\n",
    "        More parameters, but gating allows for (theoretically) improved information retention between hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c03afe-25e6-41dc-94f9-fd3b2cbaec11",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Normalization is an important part of improving generalization in ML models, by removing much of the amplitude-dependence from an input. RMS-Norm is a slight improvement to the efficiency of the original layernorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191937e7-7bd5-4e27-9842-3c0696e2501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style RMS norm\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L74\n",
    "    \"\"\"\n",
    "    def __init__(self, config, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(config.n_embd))\n",
    "\n",
    "    def norm(self, x: torch.Tensor):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.weight * self.norm(x.float()).type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3a5da-e976-4823-bdbf-642b401a3fcc",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "In a modern LM, a chain of decoder blocks uses the input signal to predict the most likely next stop in the semantic space of the model. It's composed of an attention block, and a feed forward block, which each in turn compute a residual \"move\" on top of the previous state. \n",
    "\n",
    "N blocks allows for N adjustments on top of the input, so with more layers a model can make finer moves, or take more factors into account to determine the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea6eca-0329-48a2-8d92-e4ab6fade329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard transformers block structure\n",
    "        Passed attention class, feed forward class, and layer normalization class\n",
    "\n",
    "        Forward involves attention residual followed by feed forward residual to be\n",
    "        passed to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = config.ln_class(config)\n",
    "        self.attn = config.attn_class(config)\n",
    "        self.ln_2 = config.ln_class(config)\n",
    "        self.ff = config.ff_class(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.ff(self.ln_2(x))\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5aa605-cccb-44e4-88b9-ec945e676bb8",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "A Transformer model consists of an input-processing embedding layer, which produces a \"semantic trajectory\" for the inputs, and a series of decoder blocks, which use this trajectory to predict the next step in the semantic space.\n",
    "\n",
    "To generate output, we need a de-translation layer, to convert from the position in the semantic space back to a mix of the closest tokens. We know this is the behaviour, because our embedding layer and output layer use the same weights, so the output of the \"Causal LM\" head is the similarity of each token's semantic position with that of the Transformer's final step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffca391-bc00-4a7b-87b1-a2cb86f1e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForCausalLM(Transformer):\n",
    "    \"\"\"\n",
    "    Architecturally-relevant pieces based on Karpathy's nanoGPT\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conf = config\n",
    "        \"\"\"\n",
    "        Transformer decoder stack\n",
    "            Word token embedding sequences are passed through\n",
    "            a series of processing blocks before normalization\n",
    "        \"\"\"\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # Embeddings + dropout\n",
    "            wte = config.emb_class(config),\n",
    "            \n",
    "            # Backbone of Attn->FF blocks\n",
    "            h = nn.ModuleList([\n",
    "                Block(config) for _ in range(config.n_layer)\n",
    "            ]),\n",
    "\n",
    "            # Output hidden state normalization\n",
    "            n_f = config.ln_class(config)\n",
    "        ))\n",
    "        \"\"\"\n",
    "        Language Modelling \"head\"\n",
    "            maps the hidden state output by the transformer decoder stack\n",
    "            to a token from its vocabulary\n",
    "        \"\"\"\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        \"\"\"\n",
    "        \"Weight Tying\"\n",
    "            From Karpathy, which credits https://paperswithcode.com/method/weight-tying\n",
    "            \n",
    "            By making input embeddings and output decodings map within the same space, \n",
    "            the generation of a new token is more like a \"movement\" through this latent space.\n",
    "            The word embeddings then must be clustered by frequency/semantic usage, with n_embd\n",
    "            different potential similarities or differences(/nuances).\n",
    "        \"\"\"\n",
    "        self.transformer.wte.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward method modified from Karpathy\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "\n",
    "        Calculates cross entropy loss if targets are provided. Targets are just\n",
    "        inputs shifted one to the left for foundational training (e.g. which token directly follows the input).\n",
    "    \"\"\"\n",
    "    def forward(self, idx, targets = None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "\n",
    "        hidden_state = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            hidden_state = block(hidden_state)\n",
    "        hidden_state = self.transformer.n_f(hidden_state)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(hidden_state)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(hidden_state[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42585412-1b37-46e4-acb4-67e501560ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Default config is a scaled-down version of LLama-3 for ~220M parameters\n",
    "    \"\"\"\n",
    "    max_seq_len = 1024\n",
    "    vocab_size = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer = 12\n",
    "    n_head = 16\n",
    "    n_kv_head = 8\n",
    "    n_embd = 1024\n",
    "    intermediate_size = 3584\n",
    "    dropout = 0.0\n",
    "    bias = False\n",
    "    rotary_encoding = True\n",
    "    \n",
    "    # What component classes are we using?\n",
    "    emb_class = WordTokenEmbeddings\n",
    "    attn_class = GQASelfAttention\n",
    "    ff_class = GatedMLP\n",
    "    ln_class = RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648dc12-9d40-4075-a012-bd405bdef115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download the dataset, tokenize, and create file \"block\" of tokens to grab\n",
    "sections from for training.\n",
    "Our total training length is relatively short - in 10k steps we might get 5e9/5 billion\n",
    "tokens, or about half of the stored data. Our default weight decay is also very high, so\n",
    "in general we can presume that every piece of text it sees is \"new\" and corresponds to\n",
    "validation performance.\n",
    "\n",
    "Lazy, and not good practice for wider-scope applications, but we're staying lean.\n",
    "This is not language-modelling-to-save-the-world.\n",
    "\n",
    "Downloads 10 billion tokens worth of text -- this takes several hours on gigabit\n",
    "\"\"\"\n",
    "generate_training_data(dset = \"HuggingFaceTB/smollm-corpus\", dset_name = \"cosmopedia-v2\", tokens_to_save = 1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955e8c9-cf50-468f-9410-fe8a2fe91bbc",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "The training loop uses a set of global training configurations to train a language model using the \"train.bin\" file. In each step, the gradient is accumulated for a series of entries, and backpropagated through the model, changing the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898829c-7544-4dd4-b8c2-3da3fb671464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    # Lock down training hyperparameters so they don't get changed on the fly\n",
    "    LR = g.LR\n",
    "    WD = g.WD\n",
    "    MAX_STEPS = g.MAX_STEPS\n",
    "    BATCH_S = g.BATCH_S\n",
    "    GRAD_ACCUM_STEPS = g.GRAD_ACCUM_STEPS\n",
    "    SHOW_GRADIENTS = g.SHOW_GRADIENT\n",
    "    USING_WANDB = g.USE_WANDB\n",
    "    CKPT_F = g.CHKPT_FREQ\n",
    "    INIT_CKPT = g.loadchkpt\n",
    "    print(f\"Approx. {1024*BATCH_S*GRAD_ACCUM_STEPS} tokens per optimizer step\")\n",
    "    if USING_WANDB:\n",
    "        wandb.init(project=\"OWTBot\", name=f\"LM_{model.get_num_params()//1e6}M\", \n",
    "                   config={  'nlayers':model.conf.n_layer, \n",
    "                             'nhead':model.conf.n_head,  \n",
    "                             'nkvhead':model.conf.n_kv_head, \n",
    "                             'emb_dim':model.conf.n_embd, \n",
    "                             'mix_size':model.conf.intermediate_size,\n",
    "                             'vocab_size':model.conf.vocab_size,\n",
    "                             'block_size':model.conf.block_size,\n",
    "                             'MLP':model.conf.ff_class.__name__,\n",
    "                             'Attn':model.conf.attn_class.__name__,\n",
    "                             'Embed':model.conf.emb_class.__name__,\n",
    "                             'Norm':model.conf.ln_class.__name__,\n",
    "                            })\n",
    "    \n",
    "    # Create AdamW optimizer and loss function\n",
    "    optimizer = model.configure_optimizers(weight_decay = WD,learning_rate = LR, betas=(0.9, 0.95), device_type=g.dev)\n",
    "    start = 0\n",
    "    # Attempt to load a previous checkpoint, if provided\n",
    "    if len(INIT_CKPT) > 0:\n",
    "        try:\n",
    "            \n",
    "            ckpt = torch.load(INIT_CKPT,map_location=torch.device('cpu'))\n",
    "            if \"model\" in ckpt.keys():\n",
    "                model.load_state_dict(ckpt['model'])\n",
    "            else:\n",
    "                print(f\"No bundled model!!!\\nYou'll be training a model from scratch, potentially using a different model's optimizer...\")\n",
    "            if \"optimizer\" in ckpt.keys():\n",
    "                # load optimizer state to preserve gradient information\n",
    "                optimizer.load_state_dict(ckpt['optimizer'])\n",
    "            else:\n",
    "                print(f\"No bundled optimizer!!!\\nThis will likely cause issues in training, as the gradients and momentum are missing.\")\n",
    "            if \"step\" in ckpt.keys():\n",
    "                start = ckpt['step']\n",
    "            else:\n",
    "                print(f\"No bundled step count!!! Learning rate progress will be lost, and start from zero.\")\n",
    "                \n",
    "        except:\n",
    "            print(f\"Failed to load checkpoint {INIT_CKPT}, starting from scratch\")\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    \n",
    "    # Get first batch\n",
    "    ctxs,targ = get_batch(fname = \"train.bin\", num = 0, batch_size = GRAD_ACCUM_STEPS*BATCH_S, dev = g.device)\n",
    "    \n",
    "    # Values and lists to carry through training\n",
    "    losses = [] # Loss values after each step\n",
    "    total_toks = [] # Total tokens trained on at each step\n",
    "    total_tok_ct = 0 # Running total tokens\n",
    "\n",
    "    if SHOW_GRADIENTS:\n",
    "        # Display figure to update during training\n",
    "        f,(ax1,ax2) = plt.subplots(2,1,figsize=(18,10))\n",
    "    else:\n",
    "        f,ax1 = plt.subplots(1,1,figsize=(18,5))\n",
    "        \n",
    "    dh = display.display(f, display_id=True)\n",
    "    \n",
    "    \n",
    "    for ep in trange(start, MAX_STEPS):\n",
    "        \n",
    "        if ep % CKPT_F == 0 and ep > 0:\n",
    "            checkpoint = { \n",
    "                'step': ep,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, f\"ckpt{ep}.pth\")\n",
    "            \n",
    "        bloss = 0\n",
    "        nb = 0\n",
    "        toks = 0\n",
    "    \n",
    "        # Determine and set the learning rate for this iteration\n",
    "        lr = get_lr(ep,lr_decay_iters=6e5)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        for _q in range(0,len(ctxs),BATCH_S):\n",
    "            nb += 1\n",
    "            log_probs,loss = model(ctxs[_q:_q+BATCH_S],targets=targ[_q:_q+BATCH_S])\n",
    "            # accumulate gradient\n",
    "            loss.backward()\n",
    "            bloss += loss.detach().item()\n",
    "        total_tok_ct += 1024*len(ctxs)\n",
    "        \n",
    "        # Async load next batch while doing plot rendering\n",
    "        ctxs,targ  = get_batch(fname = \"train.bin\", num = ep+1, batch_size = GRAD_ACCUM_STEPS*BATCH_S, dev = g.device)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        if SHOW_GRADIENTS:\n",
    "            if ep % 20 == 0:\n",
    "                ax2.clear()\n",
    "                plot_grad_flow(model.named_parameters(),f,ax2)\n",
    "            \n",
    "        # Keep track of the total number of tokens the model's seen    \n",
    "        total_toks.append(total_tok_ct)\n",
    "        \n",
    "        # Take optimizer step with accumulated gradient\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        losses.append(bloss/nb)\n",
    "\n",
    "        if ep % 20 == 0:\n",
    "            # Update figures\n",
    "            ax1.clear()\n",
    "            ax1.plot(total_toks,losses)\n",
    "            ax1.xaxis.set_major_formatter(ticker.FormatStrFormatter('%0.2e'))\n",
    "            plt.tight_layout()\n",
    "            dh.update(f)\n",
    "        \n",
    "        if USING_WANDB:\n",
    "            # W&B logging\n",
    "            wandb.log({\n",
    "                \"step\": ep,\n",
    "                \"total_toks\":total_toks[-1],\n",
    "                \"train/loss\": losses[-1],\n",
    "                \"lr\": lr,\n",
    "            })\n",
    "    if USING_WANDB:\n",
    "        wandb.finish()\n",
    "        \n",
    "    checkpoint = { \n",
    "        'step': ep,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, f\"ckpt{ep+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54bdd9-4a35-43bf-8391-53f39d758e34",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "Some different potential configurations that define some different model architectures based on the blocks defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43c141-24bb-4b9d-a1a8-91d4974616b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default config produces a ~220M parameter model based on\n",
    "the LLama-3 architecture\n",
    "\"\"\"\n",
    "conf_default = Config()\n",
    "\n",
    "\"\"\"\n",
    "Very small 135M model based on SmolLM-135M:\n",
    "https://huggingface.co/HuggingFaceTB/SmolLM-135M\n",
    "\"\"\"\n",
    "conf_smol = Config()\n",
    "conf_smol.n_layer = 30\n",
    "conf_smol.n_embd = 576\n",
    "conf_smol.n_head = 9\n",
    "conf_smol.n_kv_head = 3\n",
    "conf_smol.intermediate_size = 1536\n",
    "\n",
    "\"\"\"\n",
    "\"Medium-sized\" ~1B config tailored to juuust fit in 24Gb VRAM\n",
    "with batch-size of 8 during training\n",
    "\"\"\"\n",
    "conf_med = Config()\n",
    "conf_med.n_layer = 17\n",
    "conf_med.n_embd = 2048\n",
    "conf_med.n_head = 32\n",
    "conf_med.n_kv_head = 8\n",
    "conf_med.intermediate_size = 7168\n",
    "\n",
    "\"\"\"\n",
    "Reproduction of LLama-3 7B, from model specific config\n",
    "\"\"\"\n",
    "conf_llama3_7b = Config()\n",
    "conf_llama3_7b.n_layer = 32\n",
    "conf_llama3_7b.n_head = 32\n",
    "conf_llama3_7b.n_kv_head = 8\n",
    "conf_llama3_7b.n_embd = 4096\n",
    "conf_llama3_7b.intermediate_size = 14336\n",
    "\n",
    "\"\"\"\n",
    "Reproduction of GPT-2 124M, based on default config in Karpathy's nanoGPT\n",
    "\"\"\"\n",
    "conf_gpt2 = Config()\n",
    "conf_gpt2.n_layer = 12\n",
    "conf_gpt2.n_head = 12\n",
    "conf_gpt2.n_kv_head = 12\n",
    "conf_gpt2.n_embd = 768\n",
    "conf_gpt2.intermediate_size = 3072\n",
    "conf_gpt2.rotary_encoding = False\n",
    "conf_gpt2.emb_class = AbsolutePositionalEmbeddings\n",
    "conf_gpt2.attn_class = MHASelfAttention\n",
    "conf_gpt2.ff_class = MLP\n",
    "conf_gpt2.ln_class = RMSNorm\n",
    "conf_gpt2.bias = True\n",
    "\n",
    "\"\"\"\n",
    "Semi-reproduction of the RNN-based LM \"Hawk\" from\n",
    "https://arxiv.org/pdf/2402.19427\n",
    "YMMV in terms of quality, the paper does a poor job\n",
    "of enabling reproduceability (e.g. no training hyperparams, \n",
    "certain details are handwaved away)\n",
    "\"\"\"\n",
    "conf_hawk = Config()\n",
    "conf_hawk.n_layer = 6\n",
    "conf_hawk.n_embd = 768\n",
    "conf_hawk.intermediate_size = 3072\n",
    "conf_hawk.rglru_hidden_size = 1536\n",
    "conf_hawk.rotary_encoding = False\n",
    "conf_hawk.emb_class = WordTokenEmbeddings\n",
    "conf_hawk.attn_class = RGLRURecurrentBlock\n",
    "conf_hawk.ff_class = GatedMLP\n",
    "conf_hawk.ln_class = RMSNorm\n",
    "conf_hawk.bias = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f746c-4c24-4f20-b3ee-26335d9d7a89",
   "metadata": {},
   "source": [
    "## Let 'er rip!\n",
    "Here we (finally) 1. Modify the training configuration, 2. create the model, 3. tell PyTorch to \"compile\" it, to optimize the modules, and 4. perform the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5a0e8-cd90-49b8-bf40-772bfdd2e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.train_control_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431291e4-4740-4ffd-b6d6-2b6d5a5cfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerForCausalLM(conf_default).to(g.device, dtype=g.dtype)\n",
    "# output an architectural summary - n.b. the total number of parameters reported here\n",
    "# will have double the amount of embedding params -- remember, we've linked those two sets\n",
    "# of weights.\n",
    "summary(model, input_size=(1, 1024), dtypes=[torch.long],depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24695d7c-ade4-4e7f-ae94-9cf8afd4c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional -- compile model with torch\n",
    "# Uses torch's jit system to create custom kernels for the model\n",
    "# ideally, speeding up training and execution.\n",
    "# Anecdotally, I've found it makes the first training step take significantly longer,\n",
    "# but subsequent iterations are faster, leading to ~4 hours saved over 10k steps.\n",
    "# https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5d270-efe7-4af7-b29a-bc02ab4145c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b60cf-d530-4f92-b7b5-cf22e146f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

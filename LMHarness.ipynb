{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf24a32-9b32-4913-9fc1-221ad8b05930",
   "metadata": {},
   "source": [
    "## Notebook Organization Notes and Errata\n",
    "This notebook is meant to be both an educational tool, and lightweight-yet-fully-functional utility for training language models. These are two generally opposing goals, and this tension has motivated some of the choices and sacrifices I've made.\n",
    "\n",
    "* This notebook is organized in a bottom-up approach. Lower-level components, such as specific embedding, attention, and feed-forward classes are defined first, and used later, so that one could feasibly shift+enter their way through the entire notebook without issue. This choice was also motivated by the 'educational' aspect of this notebook - the individual components and their structure/code are important, and we build up from there.\n",
    "\n",
    "* I have presented, in many cases, several different classes for each component. This is an intentional choice. One can (hopefully, if I've done my job right) see the similarities between the methodologies in spite of their minor differences. My focus on the Llama(-esque) architecture as the \"modern\" approach is primarily dictated by the ubiquity of this architecture and their specific metholodological choices across different language models, and the clear success it has shown at *modelling language*. I've put some effort into making the components more interchangeable than their original forms, and I don't think I've introduced any significant errors (famous last words).\n",
    "\n",
    "* For clarity of code in the notebook itself, non-architectural utilities and other methods have been moved to separate files, so that the focus of the code presented here is *how* the information is modified as it flows through the model. I've also neglected to include certain optimizations and additions (e.g. KV caches, etc) to avoid adding undue complexity. If you want to use transformer-based models for any task that requires performance, you have *plenty* of other frameworks to use.\n",
    "\n",
    "* The training loop method uses several values set in globals.py (accessed via a widget interface), which it copies the values of so that they remain consistent during a single training run, even if the widget is changed. Widget controls aren't incredibly fine-grained, but are serviceable. This system will also \"allow\" you to use settings that will fail/use too much VRAM. Settings-tuning is on my radar, but is out of scope for now.\n",
    "\n",
    "* It's *very* important to note that a vast majority of the code in this notebook (and in the utility files) is adapted from other sources. I've done my best to note those sources, and to explain any modifications I've made. This is the nature of Software Development, and it's my hope that the work I've done here will empower people to dive deeper into the sources themselves, armed with a better understanding of how it's working at the lowest level.\n",
    "\n",
    "<sup>Thorold Tronrud, ML/AI Software Engineer, StarFish Medical</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dde307-ea7c-4094-b541-4685436a3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.set_per_process_memory_fraction(0.95, 0)\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import functools\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, fields, asdict\n",
    "from IPython import display\n",
    "from torchinfo import summary\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import trange\n",
    "import torch.jit as jit\n",
    "from typing import List\n",
    "import globals as g\n",
    "from modelling_utils import apply_rotary_pos_emb, repeat_kv, Transformer, clean_class_name, pretty_generate\n",
    "from training_utils import plot_grad_flow, get_lr, get_batch, generate_training_data, generate_training_data_mix, create_raw_text_dataset\n",
    "import wandb\n",
    "g.get_device()\n",
    "\n",
    "# If running on machine with multiple GPUs, you can set a specific device like this:\n",
    "g.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b116ee-9039-4a2f-98e3-381bdba062ce",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Embeddings convert a stream of discrete tokens into a trajectory through a semantic space. In many cases, these word-token-embeddings are mixed with positional embeddings -- either absolute and learned (e.g. GPT2) or sinusoidal across the sequence length (e.g. LLama, etc). Positional embeddings provide the model information about relative placement of different tokens, and distance between them, which is sometimes useful in language. \n",
    "\n",
    "\"NoPE\", or \"No Positional Embeddings\" has shown success, however, so YMMV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45a193-d735-4a86-bdfd-63a19cce58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style word-token embeddings + absolute positional embeddings\n",
    "        Combines word-token-embeddings and word-position-embeddings to transmit\n",
    "        both pieces of information throughout the rest of the model\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"AbsolutePositionalEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Validity assertions\n",
    "        assert config.max_seq_len > 0, f\"Max positional embeddings must be > 0: set max_seq_len positive\"\n",
    "        \n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "        self.max_positional_embeddings = config.max_seq_len\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.wpe = nn.Embedding(self.max_positional_embeddings, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # Batch, seq_len\n",
    "        b, seq_len = idx.size()\n",
    "        assert seq_len <= self.max_positional_embeddings, f\"Input sequence length {seq_len} is greater than max positional embeddings of {self.max_positional_embeddings}\"\n",
    "\n",
    "        tok_emb = self.wte(idx)\n",
    "        pos_emb = self.wpe(torch.arange(0, seq_len, dtype=torch.long, device=idx.device))\n",
    "        emb = self.drop(tok_emb + pos_emb)\n",
    "        return emb\n",
    "\n",
    "class WordTokenEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Word-token-embeddings only\n",
    "        For use in other architectures that handle positional embeddings\n",
    "        differently (e.g. LLama)\n",
    "    \"\"\"\n",
    "    def __str__(self):\n",
    "        return \"WordTokenEmbeddings\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embed = config.n_embd\n",
    "\n",
    "        self.wte = nn.Embedding(self.vocab_size, self.n_embed)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        tok_emb = self.wte(idx)\n",
    "        return self.drop(tok_emb)\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary embeddings - encoding token position through a sinusoidal\n",
    "    signal imprinted into the data passed to the attention heads\n",
    "    \"\"\"        \n",
    "    def __init__(self, config, differential_layer = False):\n",
    "        super().__init__()\n",
    "        if hasattr(config, \"rope_scaling_factor\"):\n",
    "            self.scaling_factor = config.scaling_factor\n",
    "        else:\n",
    "            self.scaling_factor = 1.0\n",
    "        if hasattr(config, \"qk_rope_dim\"):\n",
    "            self.dim = config.qk_rope_dim\n",
    "        else:\n",
    "            self.dim = config.n_embd//config.n_head\n",
    "        if differential_layer:\n",
    "            self.dim = self.dim//2 #differential attn\n",
    "        self.max_position_embeddings = config.max_seq_len\n",
    "        if hasattr(config, \"rope_base\"):\n",
    "            self.base = config.rope_base\n",
    "        else:\n",
    "            self.base = 100000\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        # For BC we register cos and sin cached\n",
    "        self.max_seq_len_cached = self.max_position_embeddings\n",
    "        self._set_cos_sin_cached(self.max_seq_len_cached, device = self.inv_freq.device, dtype = self.inv_freq.dtype)\n",
    "\n",
    "    def _set_cos_sin_cached(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(\n",
    "            0, self.max_seq_len_cached, device = self.inv_freq.device, dtype = self.inv_freq.dtype\n",
    "        ).unsqueeze(0)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(t.shape[0], -1, 1)\n",
    "        position_ids_expanded = t[:, None, :].float()\n",
    "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype).to(self.inv_freq.device), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype).to(self.inv_freq.device), persistent=False)\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        bs,nh,sl,embd = x.size()\n",
    "        if sl > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cached(seq_len = sl, device = x.device, dtype = x.dtype)\n",
    "        return self.cos_cached[:,:sl].to(dtype=x.dtype), self.sin_cached[:,:sl].to(dtype=x.dtype)\n",
    "\n",
    "\"\"\"\n",
    "Here be dragons:\n",
    "while RoPE can be extended infinitely, the model has only \"learned\"\n",
    "to utilize differences within the training range. To expand context\n",
    "on the fly, you can either fine-tune it, or modify the frequencies used\n",
    "by RoPE, which is what we're doing here.\n",
    "https://arxiv.org/abs/2309.00071\n",
    "\"\"\"\n",
    "\n",
    "# Inverse dim formula to find dim based on number of rotations\n",
    "def yarn_find_correction_dim(\n",
    "    num_rotations, dim, base=10000, max_position_embeddings=2048\n",
    "):\n",
    "    return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (\n",
    "        2 * math.log(base)\n",
    "    )\n",
    "\n",
    "# Find dim range bounds based on rotations\n",
    "def yarn_find_correction_range(\n",
    "    low_rot, high_rot, dim, base=10000, max_position_embeddings=2048\n",
    "):\n",
    "    low = math.floor(\n",
    "        yarn_find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
    "    )\n",
    "    high = math.ceil(\n",
    "        yarn_find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
    "    )\n",
    "    return max(low, 0), min(high, dim - 1)  # Clamp values just in case\n",
    "\n",
    "\n",
    "def yarn_get_mscale(scale=1, mscale=1):\n",
    "    if scale <= 1:\n",
    "        return 1.0\n",
    "    return 0.1 * mscale * math.log(scale) + 1.0\n",
    "\n",
    "\n",
    "def yarn_linear_ramp_mask(min, max, dim):\n",
    "    if min == max:\n",
    "        max += 0.001  # Prevent singularity\n",
    "\n",
    "    linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "    ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "    return ramp_func\n",
    "\n",
    "\n",
    "class YarnRotaryEmbedding(RotaryEmbedding):\n",
    "    \"\"\"\n",
    "    LLama-style rotary embeddings adapted from Transformers LLama source\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L94\n",
    "\n",
    "        !!Applied in the self-attention module, not as part of embedding block!!\n",
    "    \"\"\"        \n",
    "    def __init__(self, config):\n",
    "        self.original_max_position_embeddings = config.max_seq_len\n",
    "        \n",
    "        self.beta_fast = config.beta_fast if hasattr(config, \"beta_fast\") else 32\n",
    "        self.beta_slow = config.beta_slow if hasattr(config, \"beta_slow\") else 1\n",
    "        self.mscale = config.mscale if hasattr(config, \"mscale\") else 1\n",
    "        self.mscale_all_dim = config.mscale_all_dim if hasattr(config, \"mscale_all_dim\") else 0\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "    def _set_cos_sin_cached(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        \n",
    "        freq_extra = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim)).to(device=device)\n",
    "        freq_inter = 1.0 / (self.scaling_factor * self.base ** (torch.arange(0, self.dim, 2).float() / self.dim)).to(device=device)\n",
    "\n",
    "        low, high = yarn_find_correction_range (\n",
    "            self.beta_fast,\n",
    "            self.beta_slow,\n",
    "            self.dim,\n",
    "            self.base,\n",
    "            self.original_max_position_embeddings\n",
    "        )\n",
    "\n",
    "        inv_freq_mask = 1.0 - yarn_linear_ramp_mask(low, high, self.dim // 2).to(device=device, dtype = torch.float32)\n",
    "        inv_freq = freq_inter * (1 - inv_freq_mask) + freq_extra * inv_freq_mask\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        \n",
    "        t = torch.arange(\n",
    "            0, self.max_seq_len_cached, device = self.inv_freq.device, dtype = self.inv_freq.dtype\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        position_ids_expanded = t[:, None, :].float()\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(t.shape[0], -1, 1)\n",
    "        \n",
    "        freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        _mscale = float(\n",
    "            yarn_get_mscale(self.scaling_factor, self.mscale)\n",
    "            / yarn_get_mscale(self.scaling_factor, self.mscale_all_dim)\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"cos_cached\", _mscale * emb.cos().to(dtype).to(self.inv_freq.device), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", _mscale * emb.sin().to(dtype).to(self.inv_freq.device), persistent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171c121-c2e9-484e-996e-06a6bbe93faa",
   "metadata": {},
   "source": [
    "## Attention\n",
    "Attention is the main ingredient that makes Transformers so powerful. An attention block is composed of different attention \"heads\", which control the flow of information, interact between model layers, and each add their own information to the residual.\n",
    "\n",
    "Each attention head computes \"query\", \"key\", and \"value\" vectors for each input token. The attention pattern is the dot product of the query and key vectors, giving a value for the effect each token has on every other token. Finally, multiplying this token-token effect matrix by the value matrix gives a result vector for the head.\n",
    "\n",
    "The results for all heads are stacked side-by-side and passed through a final output layer, which produces the block's predicted \"change\" to the semantic position of all the tokens.\n",
    "\n",
    "[This article](https://transformer-circuits.pub/2021/framework/index.html) does an amazing job breaking down the function of attention heads, albeit with a slightly different model for their behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470da226-9029-4e02-9d7c-06a78d6d9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Multi-Head Attention with rotary embeddings\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Modified to include same manual attention fallback\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = YarnRotaryEmbedding(config)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "            self.attn_dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference in positional embeddings implementation.\n",
    "        This encodes the positions of each part of the inputs into the attention\n",
    "        matrix, as opposed to encoding it directly into the embedding trajectory\n",
    "        \"\"\"\n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "class GQASelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LLama-style Grouped Query Attention with rotary embeddings\n",
    "    https://arxiv.org/pdf/2305.13245\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L577\n",
    "\n",
    "    Causal SA is really just the case where the number of KV heads is equal to the number of Q heads,\n",
    "    but I've separated it here to have a \"simpler\" case available. Compilation *should* take care\n",
    "    of most of the performance differences w/r/t performing matrix multiplications in one go or not\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        assert config.n_head % config.n_kv_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd//self.n_head\n",
    "        self.num_key_value_heads = config.n_kv_head\n",
    "        self.num_key_value_groups = self.n_head // self.num_key_value_heads\n",
    "        \n",
    "        # Q, K, and V aren't identical in size anymore, so can't batch all in one MatMul\n",
    "        self.q_proj = nn.Linear(self.n_embd, self.n_head*self.head_dim, bias = config.bias)\n",
    "        self.k_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        self.v_proj = nn.Linear(self.n_embd, self.num_key_value_heads*self.head_dim, bias = config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        \n",
    "        self.using_rotary_encoding = False\n",
    "        if config.rotary_encoding:\n",
    "            self.rotary_emb = RotaryEmbedding(config)\n",
    "            self.using_rotary_encoding = True\n",
    "        else:\n",
    "            self.rotary_emb = None\n",
    "        \n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "                                        .view(1, 1, config.max_seq_len, config.max_seq_len))\n",
    "\n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        \n",
    "        if self.using_rotary_encoding:\n",
    "            if position_embeddings is None:\n",
    "                cos, sin = self.rotary_emb(v)\n",
    "            else:\n",
    "                cos, sin = position_embeddings\n",
    "            q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        \"\"\"\n",
    "        Primary difference with rotary MHA -- KV groups are repeated to match\n",
    "        Q dimension before being passed through SDPA\n",
    "        \"\"\"\n",
    "        k = repeat_kv(k, self.num_key_value_groups)\n",
    "        v = repeat_kv(v, self.num_key_value_groups)\n",
    "        \n",
    "        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "            \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "def lambda_init_fn(depth):\n",
    "    return 0.8 - 0.6 * math.exp(-0.3 * depth)\n",
    "\n",
    "class DifferentialAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        assert config.n_head % config.n_kv_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd//self.n_head//2\n",
    "        self.num_key_value_heads = config.n_kv_head//2\n",
    "        self.num_key_value_groups = self.n_head // self.num_key_value_heads\n",
    "        \n",
    "        # Q, K, and V aren't identical in size anymore, so can't batch all in one MatMul\n",
    "        self.q_proj = nn.Linear(self.n_embd, self.n_embd, bias = config.bias)\n",
    "        self.k_proj = nn.Linear(self.n_embd, self.n_embd//self.num_key_value_groups, bias = config.bias)\n",
    "        self.v_proj = nn.Linear(self.n_embd, self.n_embd//self.num_key_value_groups, bias = config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "\n",
    "        self.lambda_init = lambda_init_fn(config.n_layer)\n",
    "        self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim).normal_(mean=0,std=0.1))\n",
    "        self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim).normal_(mean=0,std=0.1))\n",
    "        self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim).normal_(mean=0,std=0.1))\n",
    "        self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim).normal_(mean=0,std=0.1))\n",
    "        \n",
    "        # Regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(config, differential_layer = True)\n",
    "        self.using_rotary_encoding = True\n",
    "        self.dropout = config.dropout\n",
    "        # replace boolean buffer with an index array\n",
    "        # booleans cause the PyTorch 2.0 compiler to panic because it assumes\n",
    "        # it's a changeable output shape, even if it isn't.\n",
    "        self.register_buffer(\"every_other_mask_e\", (torch.arange(0,self.n_head*2,2)))\n",
    "        self.register_buffer(\"every_other_mask_o\", (torch.arange(1,self.n_head*2,2)))\n",
    "        \n",
    "\n",
    "    def forward(self, x, position_embeddings = None):\n",
    "        B, T, C = x.size() # Batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(B, T, 2*self.n_head, self.head_dim).transpose(1,2)\n",
    "        k = k.view(B, T, 2*self.num_key_value_heads, self.head_dim).transpose(1,2)\n",
    "        v = v.view(B, T, self.num_key_value_heads, 2*self.head_dim).transpose(1,2)\n",
    "        # QK Norm\n",
    "        q,k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),))\n",
    "        cos, sin = self.rotary_emb(k)\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1)\n",
    "\n",
    "        k = torch.repeat_interleave(k, dim=1, repeats=self.num_key_value_groups)\n",
    "        v = torch.repeat_interleave(v, dim=1, repeats=self.num_key_value_groups*2)\n",
    "        \n",
    "        lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1)).type_as(q)\n",
    "        lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1)).type_as(q)\n",
    "        lambda_full = lambda_1 - lambda_2 + self.lambda_init\n",
    "\n",
    "        attn_weights = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        # attn_weight_size = B, 2xNH, T, HD\n",
    "        attn = torch.index_select(attn_weights, 1, self.every_other_mask_e) - lambda_full * torch.index_select(attn_weights, 1, self.every_other_mask_o)\n",
    "        \n",
    "        \n",
    "        attn = F.rms_norm(attn,(attn.size(-1),))\n",
    "        attn = attn * (1 - self.lambda_init)\n",
    "        attn = attn.transpose(1, 2).reshape(B, T, self.n_head * 2 * self.head_dim)\n",
    "        y = self.o_proj(attn)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc99d58-fb4c-435c-b15b-858abb9a3d60",
   "metadata": {},
   "source": [
    "## Feed Forward/MLP Block\n",
    "\n",
    "The feed-forward block is generally a shallow multi-layer perceptron (MLP) that calculates a residual on top of the output from the attention block. This further adjusts the direction of the next \"step\" through the model, although it is not [well understood](https://transformer-circuits.pub/2021/framework/index.html#additional-intuition) what these adjustments directly correspond to (or even, in fact, if they do correspond to any *specific* features outside of rare cases).\n",
    "\n",
    "The slightly more complex gated MLP provides a more elegant channel for specific pieces of information passed from the attention block to be ignored, by allowing a gate value to be zero simply by generating a negative number in the gate projection layer, which then fully cancels out information without needing to be the exact negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fe6d5-878b-4d68-a715-c7b81640dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT-2 style MLP feed-forward block\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L78\n",
    "\n",
    "        Expands embedding dimension to intermediate size, then collapses\n",
    "        back to pass to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class GatedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    (simplified) LLama-style gated MLP feed-forward block\n",
    "    https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L186\n",
    "\n",
    "        More parameters, but gating allows for (theoretically) improved information retention between hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.n_embd\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3a5da-e976-4823-bdbf-642b401a3fcc",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "In a modern LM, a chain of decoder blocks uses the input signal to predict the most likely next stop in the semantic space of the model. It's composed of an attention block, and a feed forward block, which each in turn compute a residual \"move\" on top of the previous state. \n",
    "\n",
    "N blocks allows for N adjustments on top of the input, so with more layers a model can make finer moves, or take more factors into account to determine the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea6eca-0329-48a2-8d92-e4ab6fade329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard transformers block structure\n",
    "        Passed attention class, feed forward class, and layer normalization class\n",
    "\n",
    "        Forward involves attention residual followed by feed forward residual to be\n",
    "        passed to next block in transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, config, i = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        if i < len(config.attn_class):\n",
    "            atncls = config.attn_class[i]\n",
    "        else:\n",
    "            atncls = config.attn_class[i%len(config.attn_class)]\n",
    "        self.attn = atncls(config)\n",
    "        \n",
    "        if i < len(config.ff_class):\n",
    "            ffcls = config.ff_class[i]\n",
    "        else:\n",
    "            ffcls = config.ff_class[i%len(config.ff_class)]\n",
    "            \n",
    "        self.ff = ffcls(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(F.rms_norm(x,(x.size(-1),)))\n",
    "        x = x + self.ff(F.rms_norm(x,(x.size(-1),)))\n",
    "        return x      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5aa605-cccb-44e4-88b9-ec945e676bb8",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "A Transformer model consists of an input-processing embedding layer, which produces a \"semantic trajectory\" for the inputs, and a series of decoder blocks, which use this trajectory to predict the next step in the semantic space.\n",
    "\n",
    "To generate output, we need a de-translation layer, to convert from the position in the semantic space back to a mix of the closest tokens. We know this is the behaviour, because our embedding layer and output layer use the same weights, so the output of the \"Causal LM\" head is the similarity of each token's semantic position with that of the Transformer's final step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffca391-bc00-4a7b-87b1-a2cb86f1e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForCausalLM(Transformer):\n",
    "    \"\"\"\n",
    "    Architecturally-relevant pieces based on Karpathy's nanoGPT\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conf = {field.name: getattr(obj, field.name) for field in fields(config)}\n",
    "        \"\"\"\n",
    "        Transformer decoder stack\n",
    "            Word token embedding sequences are passed through\n",
    "            a series of processing blocks before normalization\n",
    "        \"\"\"\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            # Embeddings + dropout\n",
    "            wte = config.emb_class(config),\n",
    "            # Backbone of Attn->FF blocks\n",
    "            h = nn.ModuleList([\n",
    "                Block(config, i) for i in range(config.n_layer)\n",
    "            ]),\n",
    "\n",
    "            # Output hidden state normalization\n",
    "            #n_f = config.ln_class(config)\n",
    "        ))\n",
    "        \"\"\"\n",
    "        Language Modelling \"head\"\n",
    "            maps the hidden state output by the transformer decoder stack\n",
    "            to a token from its vocabulary\n",
    "        \"\"\"\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        \"\"\"\n",
    "        \"Weight Tying\"\n",
    "            From Karpathy, which credits https://paperswithcode.com/method/weight-tying\n",
    "            \n",
    "            By making input embeddings and output decodings map within the same space, \n",
    "            the generation of a new token is more like a \"movement\" through this latent space.\n",
    "            The word embeddings then must be clustered by frequency/semantic usage, with n_embd\n",
    "            different potential similarities or differences(/nuances).\n",
    "        \"\"\"\n",
    "        self.transformer.wte.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # Set up gradient checkpointing\n",
    "        gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n",
    "        self.checkpoint_fn = functools.partial(checkpoint, **gradient_checkpointing_kwargs)\n",
    "        \n",
    "        # Report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward method modified from Karpathy\n",
    "    https://github.com/karpathy/nanoGPT/blob/master/model.py#L118\n",
    "\n",
    "        Calculates cross entropy loss if targets are provided. Targets are just\n",
    "        inputs shifted one to the left for foundational training (e.g. which token directly follows the input).\n",
    "    \"\"\"\n",
    "    def forward(self, idx, targets = None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "\n",
    "        hidden_state = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            \"\"\"\n",
    "            when training, we want to use gradient checkpointing\n",
    "            adds some extra compute overhead, but allows for much\n",
    "            larger batch sizes by recomputing certain gradients\n",
    "            during the backwards pass\n",
    "            \n",
    "            See:\n",
    "            https://pytorch.org/docs/stable/checkpoint.html\n",
    "            \"\"\"\n",
    "            if self.training: \n",
    "                hidden_state = self.checkpoint_fn(\n",
    "                    block.__call__,\n",
    "                    hidden_state\n",
    "                )\n",
    "            else:\n",
    "                hidden_state = block(hidden_state)\n",
    "            \n",
    "        hidden_state = F.rms_norm(hidden_state,(hidden_state.size(-1),))\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(hidden_state)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # Inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(hidden_state[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42585412-1b37-46e4-acb4-67e501560ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Default config is a scaled-down version of LLama-3 for ~220M parameters\n",
    "    \"\"\"\n",
    "    max_seq_len = 8192\n",
    "    vocab_size = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer = 12\n",
    "    n_head = 16\n",
    "    n_kv_head = 8\n",
    "    n_embd = 1024\n",
    "    intermediate_size = 3584\n",
    "    dropout = 0.0\n",
    "    bias = False\n",
    "    rotary_encoding = True\n",
    "    rope_base = 100000\n",
    "    \n",
    "    # What component classes are we using?\n",
    "    emb_class = WordTokenEmbeddings\n",
    "    \n",
    "    diffattn = True\n",
    "    attn_class = [MHASelfAttention]#[GQASelfAttention, DifferentialAttention]\n",
    "    \n",
    "    ff_class = [MLP]#[GatedMLP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648dc12-9d40-4075-a012-bd405bdef115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download the dataset, tokenize, and create file \"block\" of tokens to grab\n",
    "sections from for training.\n",
    "Our total training length is relatively short - in 10k steps we might get 5e9/5 billion\n",
    "tokens, or about half of the stored data. Our default weight decay is also very high, so\n",
    "in general we can presume that every piece of text it sees is \"new\" and corresponds to\n",
    "validation performance.\n",
    "\n",
    "Lazy, and not good practice for wider-scope applications, but we're staying lean.\n",
    "This is not language-modelling-to-save-the-world.\n",
    "\n",
    "Downloads 10 billion tokens worth of text -- this takes several hours on gigabit\n",
    "\"\"\"\n",
    "# create a single-sourced chunk of data\n",
    "#generate_training_data(dset = \"HuggingFaceTB/smollm-corpus\", dset_name = \"cosmopedia-v2\", tokens_to_save = 1e10)\n",
    "\n",
    "# or, mix and match to incorporate more different types of text\n",
    "generate_training_data_mix(dset = [\"neuralwork/arxiver\",\"mteb/raw_biorxiv\",\"wikimedia/wikipedia\",\"ccdv/pubmed-summarization\", \"HuggingFaceTB/smollm-corpus\",\"togethercomputer/RedPajama-Data-1T\",\"togethercomputer/RedPajama-Data-1T\", \"launch/gov_report\"], \n",
    "                               dset_name = [None, None,\"20231101.en\",\"section\", \"fineweb-edu-dedup\", \"common_crawl\", \"stackexchange\", \"plain_text\"], \n",
    "                               text_col = [\"markdown\",\"abstract\",\"text\",\"article\", \"text\", \"text\", \"text\", \"document\"],\n",
    "                               splits = [\"train\",\"train\",\"train\",\"train\",\"train\",\"train\",\"train\", \"train\"],\n",
    "                               tokens_to_save = [2145236507,21967910,4630994150,483329429, 1e10, 4e9, 5e9, 2e8],\n",
    "                               senc = \"gpt2\", num_proc = 1,\n",
    "                               out_fname = \"train.bin\", overwrite = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955e8c9-cf50-468f-9410-fe8a2fe91bbc",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "The training loop uses a set of global training configurations to train a language model using the \"train.bin\" file. In each step, the gradient is accumulated for a series of entries, and backpropagated through the model, changing the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898829c-7544-4dd4-b8c2-3da3fb671464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, seq_len = 1024, data_fname = \"train.bin\"):\n",
    "    # Lock down training hyperparameters so they don't get changed on the fly\n",
    "    LR = g.LR\n",
    "    WD = g.WD\n",
    "    MAX_STEPS = g.MAX_STEPS\n",
    "    BATCH_S = g.BATCH_S\n",
    "    GRAD_ACCUM_STEPS = g.GRAD_ACCUM_STEPS\n",
    "    SHOW_GRADIENTS = g.SHOW_GRADIENT\n",
    "    USING_WANDB = g.USE_WANDB\n",
    "    CKPT_F = g.CHKPT_FREQ\n",
    "    INIT_CKPT = g.loadchkpt\n",
    "    print(f\"Approx. {seq_len*BATCH_S*GRAD_ACCUM_STEPS} tokens per optimizer step\")\n",
    "    if USING_WANDB:\n",
    "        wandb.init(project=\"OWTBot\", name=f\"LM_{model.get_num_params()//1e6}M\", \n",
    "                   config={  'config':\"\",\n",
    "                             'LR': LR,\n",
    "                             'seq_len':seq_len,\n",
    "                             'BatchSize':BATCH_S,\n",
    "                             'Grad Accum': GRAD_ACCUM_STEPS,\n",
    "                            })\n",
    "    \n",
    "    # Create AdamW optimizer and loss function\n",
    "    # (0.9, 0.95) for adamW\n",
    "    # Now, create AdEMAMix optimizer\n",
    "    #optimizers, schedulers = model.configure_optimizers(weight_decay = WD,learning_rate = LR, use_adamw = True, betas=(0.9, 0.95), device_type=g.dev)\n",
    "    optimizer = model.configure_optimizers(weight_decay = WD,learning_rate = LR, use_adamw = False, betas=(0.9, 0.95), device_type=g.dev)\n",
    "    start = 0\n",
    "\n",
    "    \n",
    "    # Values and lists to carry through training\n",
    "    losses = [] # Loss values after each step\n",
    "    total_toks = [] # Total tokens trained on at each step\n",
    "    total_tok_ct = 0 # Running total tokens\n",
    "    \n",
    "    # Attempt to load a previous checkpoint, if provided\n",
    "    if len(INIT_CKPT) > 0:\n",
    "        try:\n",
    "            \n",
    "            ckpt = torch.load(INIT_CKPT,map_location=torch.device('cpu'))\n",
    "            if \"model\" in ckpt.keys():\n",
    "                model.load_state_dict(ckpt['model'])\n",
    "            else:\n",
    "                print(f\"No bundled model!!!\\nYou'll be training a model from scratch, potentially using a different model's optimizer...\")\n",
    "            if \"step\" in ckpt.keys():\n",
    "                start = ckpt['step']\n",
    "            else:\n",
    "                print(f\"No bundled step count!!! Learning rate progress will be lost, and start from zero.\")\n",
    "            if \"total_tokens\" in ckpt.keys():\n",
    "                total_tok_ct = ckpt['total_tokens']\n",
    "            else:\n",
    "                print(f\"No bundled training token count!!! No downstream effects, but will restart from zero.\")\n",
    "            if \"optimizer\" in ckpt.keys():\n",
    "                print(\"Loading checkpoint optimizer state\")\n",
    "                # load optimizer state to preserve gradient information\n",
    "                optimizer.load_state_dicts(ckpt['optimizer'])\n",
    "            else:\n",
    "                print(f\"No bundled optimizer!!!\\nThis will likely cause issues in training, as the gradients and momentum are missing.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load checkpoint {INIT_CKPT}, starting from scratch. {e}\")\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "    \n",
    "    # Get first batch\n",
    "    ctxs,targ = get_batch(fname = data_fname, num = 0, block_size = seq_len, batch_size = GRAD_ACCUM_STEPS*BATCH_S, dev = g.device)\n",
    "    \n",
    "\n",
    "    if SHOW_GRADIENTS:\n",
    "        # Display figure to update during training\n",
    "        f,(ax1,ax2) = plt.subplots(2,1,figsize=(18,10))\n",
    "    else:\n",
    "        f,ax1 = plt.subplots(1,1,figsize=(18,5))\n",
    "        \n",
    "    dh = display.display(f, display_id=True)\n",
    "    \n",
    "    \n",
    "    for ep in trange(start, MAX_STEPS):\n",
    "        \n",
    "        if ep % CKPT_F == 0 and ep > 0:\n",
    "            checkpoint = { \n",
    "                'step': ep,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer':optimizer.get_state_dicts(),\n",
    "                'total_tokens':total_tok_ct\n",
    "            }\n",
    "            torch.save(checkpoint, f\"ckpt{ep}.pth\")\n",
    "            \n",
    "        bloss = 0\n",
    "        nb = 0\n",
    "        toks = 0\n",
    "    \n",
    "        # Determine and set the learning rate for this iteration\n",
    "        #lr = get_lr(ep,lr_decay_iters=6e5)\n",
    "        #for param_group in optimizer.param_groups:\n",
    "        #    param_group['lr'] = lr\n",
    "        \n",
    "        for _q in range(0,len(ctxs),BATCH_S):\n",
    "            nb += 1\n",
    "            log_probs,loss = model(ctxs[_q:_q+BATCH_S],targets=targ[_q:_q+BATCH_S])\n",
    "            # accumulate gradient\n",
    "            loss.backward()\n",
    "            bloss += loss.detach().item()\n",
    "        total_tok_ct += seq_len*len(ctxs)\n",
    "        \n",
    "        # Async load next batch while doing plot rendering\n",
    "        ctxs,targ  = get_batch(fname = data_fname, num = ep+1, block_size = seq_len, batch_size = GRAD_ACCUM_STEPS*BATCH_S, dev = g.device)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        if SHOW_GRADIENTS:\n",
    "            if ep % 20 == 0:\n",
    "                ax2.clear()\n",
    "                plot_grad_flow(model.named_parameters(),f,ax2)\n",
    "            \n",
    "        # Keep track of the total number of tokens the model's seen    \n",
    "        total_toks.append(total_tok_ct)\n",
    "        \n",
    "        # Take optimizer step with accumulated gradient\n",
    "        optimizer.step()\n",
    "        #for opt, sched in zip(optimizers, schedulers):\n",
    "        #    opt.step()\n",
    "        #    sched.step()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        losses.append(bloss/nb)\n",
    "\n",
    "        if ep % 20 == 0:\n",
    "            # Update figures\n",
    "            ax1.clear()\n",
    "            ax1.plot(total_toks,losses)\n",
    "            ax1.xaxis.set_major_formatter(ticker.FormatStrFormatter('%0.2e'))\n",
    "            plt.tight_layout()\n",
    "            dh.update(f)\n",
    "        \n",
    "        if USING_WANDB:\n",
    "            # W&B logging\n",
    "            wandb.log({\n",
    "                \"step\": ep,\n",
    "                \"total_toks\":total_toks[-1],\n",
    "                \"train/loss\": losses[-1],\n",
    "                \"lr\": 0,\n",
    "            })\n",
    "    if USING_WANDB:\n",
    "        wandb.finish()\n",
    "        \n",
    "    checkpoint = { \n",
    "        'step': MAX_STEPS,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer':optimizer.get_state_dicts(),\n",
    "        'total_tokens':total_tok_ct\n",
    "    }\n",
    "    torch.save(checkpoint, f\"ckpt{ep+1}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54bdd9-4a35-43bf-8391-53f39d758e34",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "Some different potential configurations that define some different model architectures based on the blocks defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43c141-24bb-4b9d-a1a8-91d4974616b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default config produces a ~220M parameter model based on\n",
    "the LLama-3 architecture\n",
    "\"\"\"\n",
    "conf_default = Config()\n",
    "\n",
    "\"\"\"\n",
    "Very small 135M model based on SmolLM-135M:\n",
    "https://huggingface.co/HuggingFaceTB/SmolLM-135M\n",
    "\"\"\"\n",
    "conf_smol = Config()\n",
    "conf_smol.n_layer = 30\n",
    "conf_smol.n_embd = 512\n",
    "conf_smol.n_head = 8\n",
    "conf_smol.n_kv_head = 4\n",
    "conf_smol.intermediate_size = 1408\n",
    "\n",
    "\"\"\"\n",
    "Very small 230 model based on SmolLM-135M:\n",
    "https://huggingface.co/HuggingFaceTB/SmolLM-135M\n",
    "\"\"\"\n",
    "conf_smed = Config()\n",
    "conf_smed.n_layer = 30\n",
    "conf_smed.n_embd = 768\n",
    "conf_smed.n_head = 12\n",
    "conf_smed.n_kv_head = 6\n",
    "#conf_smed.qk_nope_dim = 48\n",
    "#conf_smed.qk_rope_dim = 16\n",
    "conf_smed.intermediate_size = 2048\n",
    "#conf_smed.attn_class = [SkipRopeAttention]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "conf_med = Config()\n",
    "conf_med.n_layer = 32\n",
    "conf_med.n_embd = 960\n",
    "conf_med.n_head = 15\n",
    "conf_med.n_kv_head = 5\n",
    "conf_med.qk_nope_dim = 48\n",
    "conf_med.qk_rope_dim = 16\n",
    "conf_med.intermediate_size = 2560\n",
    "\n",
    "\"\"\"\n",
    "Reproduction of LLama-3.2 1B, from model specific config\n",
    "\"\"\"\n",
    "conf_1B = Config()\n",
    "conf_1B.n_layer = 16\n",
    "conf_1B.n_embd = 2048\n",
    "conf_1B.n_head = 32\n",
    "conf_1B.n_kv_head = 8\n",
    "conf_1B.intermediate_size = 8192\n",
    "conf_1B.attn_class = [GQASelfAttention, DifferentialAttention]\n",
    "\n",
    "\"\"\"\n",
    "Reproduction of GPT-2 124M, based on default config in Karpathy's nanoGPT\n",
    "\"\"\"\n",
    "conf_gpt2 = Config()\n",
    "conf_gpt2.n_layer = 12\n",
    "conf_gpt2.n_head = 12\n",
    "conf_gpt2.n_kv_head = 12\n",
    "conf_gpt2.n_embd = 768\n",
    "conf_gpt2.intermediate_size = 3072\n",
    "conf_gpt2.rotary_encoding = False\n",
    "conf_gpt2.emb_class = AbsolutePositionalEmbeddings\n",
    "conf_gpt2.attn_class = [MHASelfAttention]\n",
    "conf_gpt2.ff_class = [MLP]\n",
    "conf_gpt2.bias = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17f746c-4c24-4f20-b3ee-26335d9d7a89",
   "metadata": {},
   "source": [
    "## Let 'er rip!\n",
    "Here we (finally) 1. Modify the training configuration, 2. create the model, 3. tell PyTorch to \"compile\" it, to optimize the modules, and 4. perform the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5664cb85-6186-41dd-9c4e-8832a9fe0744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 1024\n",
    "C = conf_smol\n",
    "\n",
    "C.max_seq_len = SEQ_LEN\n",
    "model = TransformerForCausalLM(C).to(g.device, dtype=g.dtype)\n",
    "# output an architectural summary - n.b. the total number of parameters reported here\n",
    "# will have double the amount of embedding params -- remember, we've linked those two sets\n",
    "# of weights.\n",
    "model_stats = summary(model, input_size=(1, SEQ_LEN), dtypes=[torch.long],depth=5)\n",
    "g.estimate_batch_size_globals(model_stats, seq_len=SEQ_LEN)\n",
    "print(str(model_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5a0e8-cd90-49b8-bf40-772bfdd2e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.train_control_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24695d7c-ade4-4e7f-ae94-9cf8afd4c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional -- compile model with torch\n",
    "# Uses torch's jit system to create custom kernels for the model\n",
    "# ideally, speeding up training and execution.\n",
    "# Anecdotally, I've found it makes the first training step take significantly longer,\n",
    "# but subsequent iterations are faster, leading to ~4 hours saved over 10k steps.\n",
    "# https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5d270-efe7-4af7-b29a-bc02ab4145c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(420691337)\n",
    "np.random.seed(420691337)\n",
    "train_model(model, seq_len = SEQ_LEN, data_fname = \"train.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2fd7e-5f79-492c-bfae-95fe1d539dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
